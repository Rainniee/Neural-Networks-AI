{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.1 - Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "rSAt6Flild8F",
        "JuT5PlXVljqi",
        "74i22VcdnqvN",
        "6G3sQDeeoHi7",
        "3J1WbKkvoU3Q",
        "qA8DfcwxosNh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/Neural-Networks-AI/blob/master/Assignment_2_1_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rSAt6Flild8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load tiny-imagenet-200"
      ]
    },
    {
      "metadata": {
        "id": "LgC6eNPfSA8R",
        "colab_type": "code",
        "outputId": "94483025-1f2e-4efc-c677-be698f9652af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os, zipfile, io, requests\n",
        "URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "def download_images(url):\n",
        "    r = requests.get(url, stream=True)\n",
        "    print ('Downloading ' + url )\n",
        "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    zip_ref.extractall('./')\n",
        "    zip_ref.close()\n",
        "download_images(URL)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://cs231n.stanford.edu/tiny-imagenet-200.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIsGkiUplnts",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Tiny ImageNet contains 200 image classes\n",
        "Each class has 500 training images, 50 validation images, and 50 test images \n",
        "\n",
        "1.   training dataset of 100,000 images\n",
        "2.   validation dataset of 10,000 images\n",
        "3.   test dataset of 10,000 images\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JuT5PlXVljqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split the training and validation dataset"
      ]
    },
    {
      "metadata": {
        "id": "tpNrBA7rYfAk",
        "colab_type": "code",
        "outputId": "dd25d885-d28c-4e2f-f683-06d2a0082d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3377
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import six.moves.cPickle as pickle\n",
        "\n",
        "data = {}\n",
        "data['train'] = {}\n",
        "data['val'] = {}\n",
        "data['train']['image'] = []\n",
        "data['train']['label'] = []\n",
        "data['val']['image'] = []\n",
        "data['val']['label'] = []\n",
        "\n",
        "size = (32, 32)\n",
        "\n",
        "N = 400 \n",
        "## 80% as training dataset and 20% as validation dataset in each 500 images\n",
        "\n",
        "# First load wnids\n",
        "wnids = list(map(lambda x: x.strip(), open('tiny-imagenet-200/wnids.txt').readlines()))\n",
        "\n",
        "# Split the training and validation dataset\n",
        "for i in range(len(wnids)):\n",
        "    wnid = wnids[i]\n",
        "    print (\"{}: {} / {}\".format(wnid, i + 1, len(wnids)))\n",
        "    \n",
        "    for j in range(500):\n",
        "        path = \"tiny-imagenet-200/train/{0}/images/{0}_{1}.JPEG\".format(wnid, j)\n",
        "        image = (Image.open(path).convert('RGB'))\n",
        "        image = image.resize(size, Image.ANTIALIAS)\n",
        "        image = np.array(image)\n",
        "        if j < N:\n",
        "            data['train']['image'].append(image)\n",
        "            data['train']['label'].append(i)\n",
        "        else:\n",
        "            data['val']['image'].append(image)\n",
        "            data['val']['label'].append(i)\n",
        "\n",
        "# Dump train.pkl\n",
        "pickle.dump(data, open('train.pkl', 'wb', -1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n02124075: 1 / 200\n",
            "n04067472: 2 / 200\n",
            "n04540053: 3 / 200\n",
            "n04099969: 4 / 200\n",
            "n07749582: 5 / 200\n",
            "n01641577: 6 / 200\n",
            "n02802426: 7 / 200\n",
            "n09246464: 8 / 200\n",
            "n07920052: 9 / 200\n",
            "n03970156: 10 / 200\n",
            "n03891332: 11 / 200\n",
            "n02106662: 12 / 200\n",
            "n03201208: 13 / 200\n",
            "n02279972: 14 / 200\n",
            "n02132136: 15 / 200\n",
            "n04146614: 16 / 200\n",
            "n07873807: 17 / 200\n",
            "n02364673: 18 / 200\n",
            "n04507155: 19 / 200\n",
            "n03854065: 20 / 200\n",
            "n03838899: 21 / 200\n",
            "n03733131: 22 / 200\n",
            "n01443537: 23 / 200\n",
            "n07875152: 24 / 200\n",
            "n03544143: 25 / 200\n",
            "n09428293: 26 / 200\n",
            "n03085013: 27 / 200\n",
            "n02437312: 28 / 200\n",
            "n07614500: 29 / 200\n",
            "n03804744: 30 / 200\n",
            "n04265275: 31 / 200\n",
            "n02963159: 32 / 200\n",
            "n02486410: 33 / 200\n",
            "n01944390: 34 / 200\n",
            "n09256479: 35 / 200\n",
            "n02058221: 36 / 200\n",
            "n04275548: 37 / 200\n",
            "n02321529: 38 / 200\n",
            "n02769748: 39 / 200\n",
            "n02099712: 40 / 200\n",
            "n07695742: 41 / 200\n",
            "n02056570: 42 / 200\n",
            "n02281406: 43 / 200\n",
            "n01774750: 44 / 200\n",
            "n02509815: 45 / 200\n",
            "n03983396: 46 / 200\n",
            "n07753592: 47 / 200\n",
            "n04254777: 48 / 200\n",
            "n02233338: 49 / 200\n",
            "n04008634: 50 / 200\n",
            "n02823428: 51 / 200\n",
            "n02236044: 52 / 200\n",
            "n03393912: 53 / 200\n",
            "n07583066: 54 / 200\n",
            "n04074963: 55 / 200\n",
            "n01629819: 56 / 200\n",
            "n09332890: 57 / 200\n",
            "n02481823: 58 / 200\n",
            "n03902125: 59 / 200\n",
            "n03404251: 60 / 200\n",
            "n09193705: 61 / 200\n",
            "n03637318: 62 / 200\n",
            "n04456115: 63 / 200\n",
            "n02666196: 64 / 200\n",
            "n03796401: 65 / 200\n",
            "n02795169: 66 / 200\n",
            "n02123045: 67 / 200\n",
            "n01855672: 68 / 200\n",
            "n01882714: 69 / 200\n",
            "n02917067: 70 / 200\n",
            "n02988304: 71 / 200\n",
            "n04398044: 72 / 200\n",
            "n02843684: 73 / 200\n",
            "n02423022: 74 / 200\n",
            "n02669723: 75 / 200\n",
            "n04465501: 76 / 200\n",
            "n02165456: 77 / 200\n",
            "n03770439: 78 / 200\n",
            "n02099601: 79 / 200\n",
            "n04486054: 80 / 200\n",
            "n02950826: 81 / 200\n",
            "n03814639: 82 / 200\n",
            "n04259630: 83 / 200\n",
            "n03424325: 84 / 200\n",
            "n02948072: 85 / 200\n",
            "n03179701: 86 / 200\n",
            "n03400231: 87 / 200\n",
            "n02206856: 88 / 200\n",
            "n03160309: 89 / 200\n",
            "n01984695: 90 / 200\n",
            "n03977966: 91 / 200\n",
            "n03584254: 92 / 200\n",
            "n04023962: 93 / 200\n",
            "n02814860: 94 / 200\n",
            "n01910747: 95 / 200\n",
            "n04596742: 96 / 200\n",
            "n03992509: 97 / 200\n",
            "n04133789: 98 / 200\n",
            "n03937543: 99 / 200\n",
            "n02927161: 100 / 200\n",
            "n01945685: 101 / 200\n",
            "n02395406: 102 / 200\n",
            "n02125311: 103 / 200\n",
            "n03126707: 104 / 200\n",
            "n04532106: 105 / 200\n",
            "n02268443: 106 / 200\n",
            "n02977058: 107 / 200\n",
            "n07734744: 108 / 200\n",
            "n03599486: 109 / 200\n",
            "n04562935: 110 / 200\n",
            "n03014705: 111 / 200\n",
            "n04251144: 112 / 200\n",
            "n04356056: 113 / 200\n",
            "n02190166: 114 / 200\n",
            "n03670208: 115 / 200\n",
            "n02002724: 116 / 200\n",
            "n02074367: 117 / 200\n",
            "n04285008: 118 / 200\n",
            "n04560804: 119 / 200\n",
            "n04366367: 120 / 200\n",
            "n02403003: 121 / 200\n",
            "n07615774: 122 / 200\n",
            "n04501370: 123 / 200\n",
            "n03026506: 124 / 200\n",
            "n02906734: 125 / 200\n",
            "n01770393: 126 / 200\n",
            "n04597913: 127 / 200\n",
            "n03930313: 128 / 200\n",
            "n04118538: 129 / 200\n",
            "n04179913: 130 / 200\n",
            "n04311004: 131 / 200\n",
            "n02123394: 132 / 200\n",
            "n04070727: 133 / 200\n",
            "n02793495: 134 / 200\n",
            "n02730930: 135 / 200\n",
            "n02094433: 136 / 200\n",
            "n04371430: 137 / 200\n",
            "n04328186: 138 / 200\n",
            "n03649909: 139 / 200\n",
            "n04417672: 140 / 200\n",
            "n03388043: 141 / 200\n",
            "n01774384: 142 / 200\n",
            "n02837789: 143 / 200\n",
            "n07579787: 144 / 200\n",
            "n04399382: 145 / 200\n",
            "n02791270: 146 / 200\n",
            "n03089624: 147 / 200\n",
            "n02814533: 148 / 200\n",
            "n04149813: 149 / 200\n",
            "n07747607: 150 / 200\n",
            "n03355925: 151 / 200\n",
            "n01983481: 152 / 200\n",
            "n04487081: 153 / 200\n",
            "n03250847: 154 / 200\n",
            "n03255030: 155 / 200\n",
            "n02892201: 156 / 200\n",
            "n02883205: 157 / 200\n",
            "n03100240: 158 / 200\n",
            "n02415577: 159 / 200\n",
            "n02480495: 160 / 200\n",
            "n01698640: 161 / 200\n",
            "n01784675: 162 / 200\n",
            "n04376876: 163 / 200\n",
            "n03444034: 164 / 200\n",
            "n01917289: 165 / 200\n",
            "n01950731: 166 / 200\n",
            "n03042490: 167 / 200\n",
            "n07711569: 168 / 200\n",
            "n04532670: 169 / 200\n",
            "n03763968: 170 / 200\n",
            "n07768694: 171 / 200\n",
            "n02999410: 172 / 200\n",
            "n03617480: 173 / 200\n",
            "n06596364: 174 / 200\n",
            "n01768244: 175 / 200\n",
            "n02410509: 176 / 200\n",
            "n03976657: 177 / 200\n",
            "n01742172: 178 / 200\n",
            "n03980874: 179 / 200\n",
            "n02808440: 180 / 200\n",
            "n02226429: 181 / 200\n",
            "n02231487: 182 / 200\n",
            "n02085620: 183 / 200\n",
            "n01644900: 184 / 200\n",
            "n02129165: 185 / 200\n",
            "n02699494: 186 / 200\n",
            "n03837869: 187 / 200\n",
            "n02815834: 188 / 200\n",
            "n07720875: 189 / 200\n",
            "n02788148: 190 / 200\n",
            "n02909870: 191 / 200\n",
            "n03706229: 192 / 200\n",
            "n07871810: 193 / 200\n",
            "n03447447: 194 / 200\n",
            "n02113799: 195 / 200\n",
            "n12267677: 196 / 200\n",
            "n03662601: 197 / 200\n",
            "n02841315: 198 / 200\n",
            "n07715103: 199 / 200\n",
            "n02504458: 200 / 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "74i22VcdnqvN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Preprossing the training and validation"
      ]
    },
    {
      "metadata": {
        "id": "XyykzIxLYfC8",
        "colab_type": "code",
        "outputId": "443fa324-400c-4206-f996-4d1c83ecd98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = (data['train']['image'],data['train']['label']),(data['val']['image'],data['val']['label'])\n",
        "\n",
        "num_classes=200\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_val = np.array(x_val)\n",
        "x_train =x_train/ 255\n",
        "x_val = x_val/255\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6G3sQDeeoHi7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model and modify the optimizer"
      ]
    },
    {
      "metadata": {
        "id": "m9OUD_eZYfFU",
        "colab_type": "code",
        "outputId": "75b52f06-4f21-4bd5-d5e0-4f9bb7bce664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bNJYP_pLY4jn",
        "colab_type": "code",
        "outputId": "005713d7-7a33-4071-c19f-db65569a8e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9075
        }
      },
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "\n",
        "opts = [('SGD', SGD()), ('Adagrad', Adagrad()), ('Adadelta', Adadelta()), \n",
        "        ('Adam', Adam()), ('Adamax', Adamax())]\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "data_augmentation = True\n",
        "\n",
        "num_predictions = 20\n",
        "Accuracy_list = []\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "for name,opt in opts:\n",
        "    print('Training ' + name + ' optimizer')\n",
        "    logs = \"logs/optimizer/\"+name\n",
        "    \n",
        "    tensorboard = TensorBoard(log_dir=logs)    \n",
        "    model_name = name + '_Keras_Optmizer_TinyImagenet200.h5'\n",
        "\n",
        "  # Ttrain the model using RMSprop\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "    start = time()\n",
        "    \n",
        "    if not data_augmentation:\n",
        "      print('Not using data augmentation.') \n",
        "      model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "\t\t\t          epochs=num_epochs,\n",
        "\t\t\t          validation_data=(x_val, y_val),\n",
        "\t\t\t          shuffle=True)\n",
        "  \n",
        "    else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                                 batch_size=batch_size),\n",
        "                                    epochs=num_epochs,\n",
        "                                    steps_per_epoch=500,\n",
        "                                    workers = 4,\n",
        "                                    validation_data=(x_val, y_val)\n",
        "                                    )\n",
        "        \n",
        "        \n",
        "    # Save model and weights\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "    end = time()\n",
        "    print('Time taken to run:', str(end-start))\n",
        "\n",
        "    print('Validation loss:', scores[0])\n",
        "    print('Validation accuracy:', scores[1])\n",
        "    \n",
        "    Accuracy_list.append([name,scores[1]])\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training SGD optimizer\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 22s 44ms/step - loss: 5.2988 - acc: 0.0057 - val_loss: 5.2980 - val_acc: 0.0048\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2985 - acc: 0.0055 - val_loss: 5.2975 - val_acc: 0.0069\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2979 - acc: 0.0054 - val_loss: 5.2969 - val_acc: 0.0053\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2975 - acc: 0.0056 - val_loss: 5.2957 - val_acc: 0.0062\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2957 - acc: 0.0057 - val_loss: 5.2926 - val_acc: 0.0069\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2909 - acc: 0.0063 - val_loss: 5.2837 - val_acc: 0.0085\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 5.2790 - acc: 0.0070 - val_loss: 5.2541 - val_acc: 0.0115\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2459 - acc: 0.0101 - val_loss: 5.2040 - val_acc: 0.0110\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.2094 - acc: 0.0103 - val_loss: 5.1635 - val_acc: 0.0102\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1839 - acc: 0.0106 - val_loss: 5.1373 - val_acc: 0.0129\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1545 - acc: 0.0135 - val_loss: 5.1237 - val_acc: 0.0132\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1510 - acc: 0.0126 - val_loss: 5.1075 - val_acc: 0.0157\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1439 - acc: 0.0139 - val_loss: 5.0961 - val_acc: 0.0168\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1374 - acc: 0.0126 - val_loss: 5.0913 - val_acc: 0.0214\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1215 - acc: 0.0154 - val_loss: 5.0668 - val_acc: 0.0193\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 5.1035 - acc: 0.0178 - val_loss: 5.0542 - val_acc: 0.0245\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.0933 - acc: 0.0181 - val_loss: 5.0412 - val_acc: 0.0246\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 5.0796 - acc: 0.0209 - val_loss: 5.0300 - val_acc: 0.0283\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 5.0658 - acc: 0.0208 - val_loss: 5.0061 - val_acc: 0.0328\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 5.0522 - acc: 0.0234 - val_loss: 5.0008 - val_acc: 0.0350\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 5.0207 - acc: 0.0261 - val_loss: 4.9611 - val_acc: 0.0387\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.9929 - acc: 0.0301 - val_loss: 4.9218 - val_acc: 0.0404\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.9817 - acc: 0.0275 - val_loss: 4.9010 - val_acc: 0.0445\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.9654 - acc: 0.0288 - val_loss: 4.8655 - val_acc: 0.0471\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 4.9528 - acc: 0.0313 - val_loss: 4.8644 - val_acc: 0.0472\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.9265 - acc: 0.0356 - val_loss: 4.8961 - val_acc: 0.0416\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.9061 - acc: 0.0374 - val_loss: 4.8427 - val_acc: 0.0483\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.9001 - acc: 0.0377 - val_loss: 4.8201 - val_acc: 0.0524\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 4.8821 - acc: 0.0384 - val_loss: 4.7846 - val_acc: 0.0548\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.8642 - acc: 0.0399 - val_loss: 4.7680 - val_acc: 0.0602\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 4.8438 - acc: 0.0454 - val_loss: 4.7643 - val_acc: 0.0587\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 4.8280 - acc: 0.0447 - val_loss: 4.7241 - val_acc: 0.0619\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.8192 - acc: 0.0465 - val_loss: 4.7145 - val_acc: 0.0635\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7925 - acc: 0.0474 - val_loss: 4.6846 - val_acc: 0.0660\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7961 - acc: 0.0496 - val_loss: 4.6777 - val_acc: 0.0684\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7480 - acc: 0.0519 - val_loss: 4.7050 - val_acc: 0.0636\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7460 - acc: 0.0512 - val_loss: 4.6581 - val_acc: 0.0674\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7542 - acc: 0.0560 - val_loss: 4.6249 - val_acc: 0.0696\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7333 - acc: 0.0532 - val_loss: 4.6034 - val_acc: 0.0738\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7249 - acc: 0.0567 - val_loss: 4.6004 - val_acc: 0.0747\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.6970 - acc: 0.0612 - val_loss: 4.5827 - val_acc: 0.0775\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.6801 - acc: 0.0612 - val_loss: 4.5535 - val_acc: 0.0795\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.6781 - acc: 0.0590 - val_loss: 4.5460 - val_acc: 0.0809\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.6814 - acc: 0.0605 - val_loss: 4.5266 - val_acc: 0.0853\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.6366 - acc: 0.0629 - val_loss: 4.5379 - val_acc: 0.0803\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.6239 - acc: 0.0649 - val_loss: 4.4941 - val_acc: 0.0896\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.6131 - acc: 0.0672 - val_loss: 4.4592 - val_acc: 0.0914\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.6148 - acc: 0.0688 - val_loss: 4.4541 - val_acc: 0.0951\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.5735 - acc: 0.0722 - val_loss: 4.4536 - val_acc: 0.0897\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.5737 - acc: 0.0733 - val_loss: 4.4120 - val_acc: 0.0970\n",
            "Saved trained model at /content/saved_models/SGD_Keras_Optmizer_TinyImagenet200.h5 \n",
            "20000/20000 [==============================] - 4s 194us/step\n",
            "Time taken to run: 857.0533356666565\n",
            "Validation loss: 4.4119902523040775\n",
            "Validation accuracy: 0.09705\n",
            "Training Adagrad optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 4.8169 - acc: 0.0459 - val_loss: 4.5024 - val_acc: 0.0790\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.5703 - acc: 0.0680 - val_loss: 4.3828 - val_acc: 0.0960\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.4695 - acc: 0.0816 - val_loss: 4.3117 - val_acc: 0.1056\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.4441 - acc: 0.0887 - val_loss: 4.2291 - val_acc: 0.1157\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.3992 - acc: 0.0884 - val_loss: 4.2040 - val_acc: 0.1207\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.3085 - acc: 0.1036 - val_loss: 4.1601 - val_acc: 0.1275\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.3083 - acc: 0.1031 - val_loss: 4.1102 - val_acc: 0.1341\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.2568 - acc: 0.1113 - val_loss: 4.0906 - val_acc: 0.1371\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 4.2057 - acc: 0.1164 - val_loss: 4.0705 - val_acc: 0.1396\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.2344 - acc: 0.1128 - val_loss: 4.0413 - val_acc: 0.1459\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 4.1601 - acc: 0.1244 - val_loss: 4.0104 - val_acc: 0.1476\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1502 - acc: 0.1237 - val_loss: 4.0000 - val_acc: 0.1515\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1236 - acc: 0.1234 - val_loss: 4.0001 - val_acc: 0.1522\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1429 - acc: 0.1224 - val_loss: 3.9723 - val_acc: 0.1560\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 4.1191 - acc: 0.1326 - val_loss: 3.9470 - val_acc: 0.1607\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 4.0720 - acc: 0.1359 - val_loss: 3.9257 - val_acc: 0.1626\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0616 - acc: 0.1350 - val_loss: 3.9244 - val_acc: 0.1615\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0286 - acc: 0.1386 - val_loss: 3.8788 - val_acc: 0.1698\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0474 - acc: 0.1373 - val_loss: 3.9012 - val_acc: 0.1661\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0655 - acc: 0.1380 - val_loss: 3.8912 - val_acc: 0.1663\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0049 - acc: 0.1471 - val_loss: 3.8554 - val_acc: 0.1742\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9891 - acc: 0.1470 - val_loss: 3.8203 - val_acc: 0.1785\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9929 - acc: 0.1496 - val_loss: 3.8029 - val_acc: 0.1854\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.9730 - acc: 0.1485 - val_loss: 3.7971 - val_acc: 0.1820\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9660 - acc: 0.1476 - val_loss: 3.8126 - val_acc: 0.1830\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9201 - acc: 0.1559 - val_loss: 3.7673 - val_acc: 0.1875\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9425 - acc: 0.1531 - val_loss: 3.7863 - val_acc: 0.1853\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9361 - acc: 0.1542 - val_loss: 3.7821 - val_acc: 0.1830\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.9074 - acc: 0.1551 - val_loss: 3.7442 - val_acc: 0.1921\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9202 - acc: 0.1538 - val_loss: 3.7512 - val_acc: 0.1914\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8876 - acc: 0.1640 - val_loss: 3.7450 - val_acc: 0.1885\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8760 - acc: 0.1618 - val_loss: 3.7756 - val_acc: 0.1855\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8949 - acc: 0.1614 - val_loss: 3.7179 - val_acc: 0.1964\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8841 - acc: 0.1600 - val_loss: 3.7165 - val_acc: 0.1973\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8600 - acc: 0.1684 - val_loss: 3.7474 - val_acc: 0.1910\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8484 - acc: 0.1699 - val_loss: 3.7098 - val_acc: 0.1945\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8500 - acc: 0.1689 - val_loss: 3.7052 - val_acc: 0.1981\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8270 - acc: 0.1727 - val_loss: 3.6801 - val_acc: 0.2002\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8346 - acc: 0.1694 - val_loss: 3.6959 - val_acc: 0.1999\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8383 - acc: 0.1716 - val_loss: 3.6911 - val_acc: 0.2007\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7860 - acc: 0.1767 - val_loss: 3.6530 - val_acc: 0.2049\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8167 - acc: 0.1683 - val_loss: 3.6538 - val_acc: 0.2050\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8001 - acc: 0.1750 - val_loss: 3.6414 - val_acc: 0.2074\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7960 - acc: 0.1751 - val_loss: 3.6305 - val_acc: 0.2097\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7915 - acc: 0.1791 - val_loss: 3.6436 - val_acc: 0.2080\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7719 - acc: 0.1797 - val_loss: 3.6432 - val_acc: 0.2071\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7562 - acc: 0.1836 - val_loss: 3.6477 - val_acc: 0.2064\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7508 - acc: 0.1813 - val_loss: 3.6675 - val_acc: 0.2026\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7515 - acc: 0.1784 - val_loss: 3.6292 - val_acc: 0.2077\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7768 - acc: 0.1799 - val_loss: 3.5984 - val_acc: 0.2150\n",
            "Saved trained model at /content/saved_models/Adagrad_Keras_Optmizer_TinyImagenet200.h5 \n",
            "20000/20000 [==============================] - 3s 139us/step\n",
            "Time taken to run: 862.5575287342072\n",
            "Validation loss: 3.59842714805603\n",
            "Validation accuracy: 0.21495\n",
            "Training Adadelta optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 4.0598 - acc: 0.1379 - val_loss: 3.8985 - val_acc: 0.1671\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 4.0606 - acc: 0.1396 - val_loss: 3.8725 - val_acc: 0.1711\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0626 - acc: 0.1412 - val_loss: 3.8357 - val_acc: 0.1751\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0098 - acc: 0.1448 - val_loss: 3.9221 - val_acc: 0.1652\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0023 - acc: 0.1466 - val_loss: 3.8072 - val_acc: 0.1815\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.9386 - acc: 0.1544 - val_loss: 3.7170 - val_acc: 0.1946\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9311 - acc: 0.1577 - val_loss: 3.8516 - val_acc: 0.1700\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.9414 - acc: 0.1596 - val_loss: 3.7602 - val_acc: 0.1920\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 21s 43ms/step - loss: 3.9421 - acc: 0.1604 - val_loss: 3.6773 - val_acc: 0.2016\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.9370 - acc: 0.1581 - val_loss: 3.6951 - val_acc: 0.1973\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8769 - acc: 0.1667 - val_loss: 3.7228 - val_acc: 0.1938\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.8835 - acc: 0.1695 - val_loss: 3.7058 - val_acc: 0.1902\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8523 - acc: 0.1727 - val_loss: 3.6830 - val_acc: 0.1960\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.8707 - acc: 0.1654 - val_loss: 3.6932 - val_acc: 0.1960\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8614 - acc: 0.1661 - val_loss: 3.7346 - val_acc: 0.1899\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.8289 - acc: 0.1715 - val_loss: 3.6273 - val_acc: 0.2060\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.8264 - acc: 0.1757 - val_loss: 3.7178 - val_acc: 0.1887\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8311 - acc: 0.1788 - val_loss: 3.5889 - val_acc: 0.2127\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8299 - acc: 0.1723 - val_loss: 3.7365 - val_acc: 0.1943\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.8083 - acc: 0.1811 - val_loss: 3.7917 - val_acc: 0.1825\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7674 - acc: 0.1854 - val_loss: 3.6252 - val_acc: 0.2114\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8126 - acc: 0.1785 - val_loss: 3.5726 - val_acc: 0.2160\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.8148 - acc: 0.1769 - val_loss: 3.5607 - val_acc: 0.2150\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.8081 - acc: 0.1811 - val_loss: 3.6996 - val_acc: 0.2061\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7951 - acc: 0.1796 - val_loss: 3.6295 - val_acc: 0.2077\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7908 - acc: 0.1847 - val_loss: 3.5368 - val_acc: 0.2225\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7553 - acc: 0.1866 - val_loss: 3.5519 - val_acc: 0.2180\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7712 - acc: 0.1841 - val_loss: 3.5619 - val_acc: 0.2152\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7970 - acc: 0.1836 - val_loss: 3.5161 - val_acc: 0.2255\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7960 - acc: 0.1844 - val_loss: 3.5984 - val_acc: 0.2067\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7484 - acc: 0.1901 - val_loss: 3.5863 - val_acc: 0.2104\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7649 - acc: 0.1844 - val_loss: 3.6796 - val_acc: 0.2009\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7698 - acc: 0.1847 - val_loss: 3.6686 - val_acc: 0.2008\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7739 - acc: 0.1838 - val_loss: 3.5620 - val_acc: 0.2173\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7845 - acc: 0.1832 - val_loss: 3.5830 - val_acc: 0.2127\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7362 - acc: 0.1927 - val_loss: 3.6190 - val_acc: 0.2058\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 3.7636 - acc: 0.1865 - val_loss: 3.4992 - val_acc: 0.2268\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7665 - acc: 0.1881 - val_loss: 3.6040 - val_acc: 0.2105\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.7776 - acc: 0.1849 - val_loss: 3.5667 - val_acc: 0.2167\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7496 - acc: 0.1867 - val_loss: 3.5350 - val_acc: 0.2206\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.7601 - acc: 0.1883 - val_loss: 3.5327 - val_acc: 0.2203\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7295 - acc: 0.1903 - val_loss: 3.5919 - val_acc: 0.2091\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7661 - acc: 0.1909 - val_loss: 3.5097 - val_acc: 0.2271\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7741 - acc: 0.1863 - val_loss: 3.5131 - val_acc: 0.2275\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7608 - acc: 0.1877 - val_loss: 3.6121 - val_acc: 0.2122\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7507 - acc: 0.1923 - val_loss: 3.5264 - val_acc: 0.2208\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7234 - acc: 0.1963 - val_loss: 3.5602 - val_acc: 0.2213\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7625 - acc: 0.1875 - val_loss: 3.5728 - val_acc: 0.2155\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7513 - acc: 0.1909 - val_loss: 3.4953 - val_acc: 0.2281\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7574 - acc: 0.1868 - val_loss: 3.4617 - val_acc: 0.2329\n",
            "Saved trained model at /content/saved_models/Adadelta_Keras_Optmizer_TinyImagenet200.h5 \n",
            "20000/20000 [==============================] - 3s 146us/step\n",
            "Time taken to run: 893.7286961078644\n",
            "Validation loss: 3.4617303440093994\n",
            "Validation accuracy: 0.23295\n",
            "Training Adam optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 19s 38ms/step - loss: 3.7531 - acc: 0.1899 - val_loss: 3.5827 - val_acc: 0.2142\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7720 - acc: 0.1876 - val_loss: 3.5827 - val_acc: 0.2089\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 3.7921 - acc: 0.1837 - val_loss: 3.5093 - val_acc: 0.2268\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.7580 - acc: 0.1837 - val_loss: 3.5559 - val_acc: 0.2150\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7563 - acc: 0.1904 - val_loss: 3.5201 - val_acc: 0.2192\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7131 - acc: 0.1937 - val_loss: 3.5623 - val_acc: 0.2156\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7136 - acc: 0.1953 - val_loss: 3.6136 - val_acc: 0.2093\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7308 - acc: 0.1884 - val_loss: 3.5275 - val_acc: 0.2203\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7231 - acc: 0.1871 - val_loss: 3.5188 - val_acc: 0.2252\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7486 - acc: 0.1871 - val_loss: 3.4910 - val_acc: 0.2239\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6755 - acc: 0.1971 - val_loss: 3.5218 - val_acc: 0.2235\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6731 - acc: 0.1987 - val_loss: 3.5330 - val_acc: 0.2208\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7118 - acc: 0.1959 - val_loss: 3.6140 - val_acc: 0.2155\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.7287 - acc: 0.1903 - val_loss: 3.4873 - val_acc: 0.2309\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.7062 - acc: 0.1931 - val_loss: 3.5137 - val_acc: 0.2281\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6411 - acc: 0.2043 - val_loss: 3.4200 - val_acc: 0.2419\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.6752 - acc: 0.1990 - val_loss: 3.5478 - val_acc: 0.2280\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6795 - acc: 0.1921 - val_loss: 3.4536 - val_acc: 0.2354\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6700 - acc: 0.1966 - val_loss: 3.4891 - val_acc: 0.2321\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7024 - acc: 0.1959 - val_loss: 3.5907 - val_acc: 0.2168\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6373 - acc: 0.2061 - val_loss: 3.5159 - val_acc: 0.2233\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6586 - acc: 0.1985 - val_loss: 3.4992 - val_acc: 0.2276\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6681 - acc: 0.1947 - val_loss: 3.5362 - val_acc: 0.2197\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6705 - acc: 0.1978 - val_loss: 3.4543 - val_acc: 0.2386\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6532 - acc: 0.1958 - val_loss: 3.5185 - val_acc: 0.2253\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6362 - acc: 0.1984 - val_loss: 3.5346 - val_acc: 0.2203\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6358 - acc: 0.2037 - val_loss: 3.4219 - val_acc: 0.2402\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6374 - acc: 0.2016 - val_loss: 3.5260 - val_acc: 0.2225\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6368 - acc: 0.1996 - val_loss: 3.5192 - val_acc: 0.2246\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6715 - acc: 0.1954 - val_loss: 3.4665 - val_acc: 0.2324\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6330 - acc: 0.2044 - val_loss: 3.5728 - val_acc: 0.2145\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6268 - acc: 0.2044 - val_loss: 3.4546 - val_acc: 0.2344\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6313 - acc: 0.1989 - val_loss: 3.5370 - val_acc: 0.2263\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.6391 - acc: 0.2026 - val_loss: 3.4978 - val_acc: 0.2294\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.6412 - acc: 0.2016 - val_loss: 3.4581 - val_acc: 0.2343\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.5962 - acc: 0.2044 - val_loss: 3.6556 - val_acc: 0.2087\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.5952 - acc: 0.2038 - val_loss: 3.4927 - val_acc: 0.2304\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6430 - acc: 0.2006 - val_loss: 3.4802 - val_acc: 0.2299\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6517 - acc: 0.1983 - val_loss: 3.4811 - val_acc: 0.2319\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6497 - acc: 0.1974 - val_loss: 3.4907 - val_acc: 0.2265\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.5885 - acc: 0.2085 - val_loss: 3.4319 - val_acc: 0.2397\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6335 - acc: 0.1988 - val_loss: 3.5014 - val_acc: 0.2308\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6072 - acc: 0.2052 - val_loss: 3.6402 - val_acc: 0.2108\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6399 - acc: 0.2016 - val_loss: 3.4952 - val_acc: 0.2299\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6250 - acc: 0.2049 - val_loss: 3.4907 - val_acc: 0.2286\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6040 - acc: 0.2085 - val_loss: 3.4972 - val_acc: 0.2238\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.5862 - acc: 0.2083 - val_loss: 3.5087 - val_acc: 0.2261\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6275 - acc: 0.2027 - val_loss: 3.5838 - val_acc: 0.2209\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.6141 - acc: 0.2067 - val_loss: 3.4710 - val_acc: 0.2306\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.6428 - acc: 0.1985 - val_loss: 3.5454 - val_acc: 0.2238\n",
            "Saved trained model at /content/saved_models/Adam_Keras_Optmizer_TinyImagenet200.h5 \n",
            "20000/20000 [==============================] - 3s 145us/step\n",
            "Time taken to run: 896.8075165748596\n",
            "Validation loss: 3.545350364971161\n",
            "Validation accuracy: 0.22385\n",
            "Training Adamax optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 19s 38ms/step - loss: 3.5429 - acc: 0.2193 - val_loss: 3.3706 - val_acc: 0.2505\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4873 - acc: 0.2220 - val_loss: 3.4180 - val_acc: 0.2406\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.4801 - acc: 0.2257 - val_loss: 3.3868 - val_acc: 0.2497\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4442 - acc: 0.2348 - val_loss: 3.3955 - val_acc: 0.2469\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4958 - acc: 0.2265 - val_loss: 3.4010 - val_acc: 0.2437\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4610 - acc: 0.2257 - val_loss: 3.3772 - val_acc: 0.2541\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4476 - acc: 0.2267 - val_loss: 3.3416 - val_acc: 0.2577\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4460 - acc: 0.2291 - val_loss: 3.3679 - val_acc: 0.2532\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4562 - acc: 0.2309 - val_loss: 3.3283 - val_acc: 0.2593\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4152 - acc: 0.2301 - val_loss: 3.4274 - val_acc: 0.2407\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4057 - acc: 0.2351 - val_loss: 3.4911 - val_acc: 0.2384\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4287 - acc: 0.2346 - val_loss: 3.3639 - val_acc: 0.2547\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4323 - acc: 0.2343 - val_loss: 3.3690 - val_acc: 0.2541\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4334 - acc: 0.2300 - val_loss: 3.3801 - val_acc: 0.2483\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4184 - acc: 0.2361 - val_loss: 3.4134 - val_acc: 0.2486\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.3862 - acc: 0.2394 - val_loss: 3.2972 - val_acc: 0.2633\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3908 - acc: 0.2397 - val_loss: 3.3226 - val_acc: 0.2590\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4208 - acc: 0.2343 - val_loss: 3.3817 - val_acc: 0.2503\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.4356 - acc: 0.2303 - val_loss: 3.3437 - val_acc: 0.2578\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4084 - acc: 0.2324 - val_loss: 3.2823 - val_acc: 0.2658\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3819 - acc: 0.2410 - val_loss: 3.3537 - val_acc: 0.2562\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.4035 - acc: 0.2404 - val_loss: 3.3377 - val_acc: 0.2613\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3851 - acc: 0.2378 - val_loss: 3.3401 - val_acc: 0.2550\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.4028 - acc: 0.2394 - val_loss: 3.4043 - val_acc: 0.2486\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3986 - acc: 0.2364 - val_loss: 3.3175 - val_acc: 0.2650\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3555 - acc: 0.2441 - val_loss: 3.4089 - val_acc: 0.2494\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3605 - acc: 0.2469 - val_loss: 3.3511 - val_acc: 0.2559\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3865 - acc: 0.2369 - val_loss: 3.3224 - val_acc: 0.2594\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 3.3927 - acc: 0.2436 - val_loss: 3.3447 - val_acc: 0.2558\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3997 - acc: 0.2356 - val_loss: 3.3378 - val_acc: 0.2596\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3890 - acc: 0.2403 - val_loss: 3.3936 - val_acc: 0.2500\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 3.3421 - acc: 0.2456 - val_loss: 3.3190 - val_acc: 0.2628\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3801 - acc: 0.2406 - val_loss: 3.3998 - val_acc: 0.2512\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.3885 - acc: 0.2392 - val_loss: 3.3625 - val_acc: 0.2558\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3937 - acc: 0.2386 - val_loss: 3.2673 - val_acc: 0.2700\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 3.3862 - acc: 0.2396 - val_loss: 3.2743 - val_acc: 0.2680\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.3742 - acc: 0.2428 - val_loss: 3.2808 - val_acc: 0.2677\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 3.3540 - acc: 0.2459 - val_loss: 3.2797 - val_acc: 0.2716\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 3.3602 - acc: 0.2444 - val_loss: 3.2845 - val_acc: 0.2666\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3800 - acc: 0.2434 - val_loss: 3.3556 - val_acc: 0.2575\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.3405 - acc: 0.2461 - val_loss: 3.2927 - val_acc: 0.2646\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3574 - acc: 0.2452 - val_loss: 3.3097 - val_acc: 0.2584\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 3.3881 - acc: 0.2449 - val_loss: 3.2644 - val_acc: 0.2694\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3642 - acc: 0.2373 - val_loss: 3.3775 - val_acc: 0.2538\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3627 - acc: 0.2408 - val_loss: 3.3637 - val_acc: 0.2561\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3318 - acc: 0.2498 - val_loss: 3.3741 - val_acc: 0.2517\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3483 - acc: 0.2480 - val_loss: 3.2321 - val_acc: 0.2754\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3310 - acc: 0.2484 - val_loss: 3.2723 - val_acc: 0.2683\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3465 - acc: 0.2483 - val_loss: 3.3027 - val_acc: 0.2638\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.3733 - acc: 0.2459 - val_loss: 3.3930 - val_acc: 0.2550\n",
            "Saved trained model at /content/saved_models/Adamax_Keras_Optmizer_TinyImagenet200.h5 \n",
            "20000/20000 [==============================] - 3s 147us/step\n",
            "Time taken to run: 890.2113757133484\n",
            "Validation loss: 3.3930377967834473\n",
            "Validation accuracy: 0.25495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g53cBIJHY4mI",
        "colab_type": "code",
        "outputId": "8a960dac-ac14-4cec-b366-6808c43d8bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "Accuracy_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['SGD', 0.09705],\n",
              " ['Adagrad', 0.21495],\n",
              " ['Adadelta', 0.23295],\n",
              " ['Adam', 0.22385],\n",
              " ['Adamax', 0.25495]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "xLGk9u3_WSPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*   Adamax resulted in the max accuracy\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3J1WbKkvoU3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model and modify the activation"
      ]
    },
    {
      "metadata": {
        "id": "SX5xzuvXY4oy",
        "colab_type": "code",
        "outputId": "d7fa77df-599d-4306-c23d-cb1a017e0277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7325
        }
      },
      "cell_type": "code",
      "source": [
        "activations = ['relu','elu','tanh']\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "data_augmentation = True\n",
        "\n",
        "num_predictions = 20\n",
        "Accuracy_list = []\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "for acti in activations:\n",
        "  print('Training' + acti + ' Activation function')\n",
        "  \n",
        "  logs = \"logs/activation/\"+name\n",
        "    \n",
        "  model_name = acti + '_Keras_Activation_TinyImagenet200.h5'\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation(acti))\n",
        "\n",
        "  model.summary()\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adamax(), metrics=['accuracy'])\n",
        "\n",
        "  start = time()\n",
        "    \n",
        "  if not data_augmentation:\n",
        "    print('Not using data augmentation.') \n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "\t\t\t        epochs=num_epochs,\n",
        "\t\t\t        validation_data=(x_val, y_val),\n",
        "\t\t\t        shuffle=True)\n",
        "  \n",
        "  else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                             batch_size=batch_size),\n",
        "                                epochs=num_epochs,\n",
        "                                steps_per_epoch=500,\n",
        "                                workers = 4,\n",
        "                                validation_data=(x_val, y_val)\n",
        "                                )\n",
        "        \n",
        "        \n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "    end = time()\n",
        "    print('Time taken to run:', str(end-start))\n",
        "\n",
        "    print('Validation loss:', scores[0])\n",
        "    print('Validation accuracy:', scores[1])\n",
        "    \n",
        "    Accuracy_list.append([name,scores[1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainingrelu Activation function\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 19s 37ms/step - loss: 14.7334 - acc: 0.0053 - val_loss: 14.4513 - val_acc: 0.0050\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 14.2513 - acc: 0.0046 - val_loss: 14.0915 - val_acc: 0.0055\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 13.9388 - acc: 0.0041 - val_loss: 13.7618 - val_acc: 0.0054\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 13.7246 - acc: 0.0053 - val_loss: 13.5334 - val_acc: 0.0047\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 13.4680 - acc: 0.0051 - val_loss: 13.2991 - val_acc: 0.0050\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 13.2457 - acc: 0.0054 - val_loss: 13.0772 - val_acc: 0.0050\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 13.0825 - acc: 0.0051 - val_loss: 12.9669 - val_acc: 0.0053\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 13.0148 - acc: 0.0052 - val_loss: 12.9144 - val_acc: 0.0058\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 12.8934 - acc: 0.0047 - val_loss: 12.7948 - val_acc: 0.0052\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 12.7991 - acc: 0.0055 - val_loss: 12.6918 - val_acc: 0.0050\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 12.6409 - acc: 0.0051 - val_loss: 12.6039 - val_acc: 0.0051\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 12.6087 - acc: 0.0051 - val_loss: 12.6196 - val_acc: 0.0050\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 12.5193 - acc: 0.0054 - val_loss: 12.4637 - val_acc: 0.0057\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 12.5576 - acc: 0.0039 - val_loss: 12.4069 - val_acc: 0.0073\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 12.3173 - acc: 0.0047 - val_loss: 12.1509 - val_acc: 0.0064\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 12.0064 - acc: 0.0053 - val_loss: 11.9757 - val_acc: 0.0050\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 12.0317 - acc: 0.0038 - val_loss: 11.9670 - val_acc: 0.0050\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.9501 - acc: 0.0045 - val_loss: 11.9653 - val_acc: 0.0050\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8874 - acc: 0.0047 - val_loss: 11.9521 - val_acc: 0.0108\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.9746 - acc: 0.0053 - val_loss: 11.8686 - val_acc: 0.0073\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8950 - acc: 0.0079 - val_loss: 11.8395 - val_acc: 0.0104\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8346 - acc: 0.0074 - val_loss: 11.8492 - val_acc: 0.0070\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8784 - acc: 0.0091 - val_loss: 11.8473 - val_acc: 0.0082\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.8974 - acc: 0.0073 - val_loss: 11.7823 - val_acc: 0.0124\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.7548 - acc: 0.0085 - val_loss: 11.7895 - val_acc: 0.0137\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.8131 - acc: 0.0089 - val_loss: 11.7740 - val_acc: 0.0118\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.8020 - acc: 0.0087 - val_loss: 11.7623 - val_acc: 0.0140\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7942 - acc: 0.0091 - val_loss: 11.8702 - val_acc: 0.0060\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7728 - acc: 0.0092 - val_loss: 11.7882 - val_acc: 0.0114\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8275 - acc: 0.0067 - val_loss: 11.7337 - val_acc: 0.0106\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.7226 - acc: 0.0102 - val_loss: 11.7189 - val_acc: 0.0129\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 11.7839 - acc: 0.0079 - val_loss: 11.7247 - val_acc: 0.0109\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.6818 - acc: 0.0088 - val_loss: 11.7308 - val_acc: 0.0140\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8290 - acc: 0.0104 - val_loss: 11.8508 - val_acc: 0.0086\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7879 - acc: 0.0108 - val_loss: 11.7428 - val_acc: 0.0067\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.8157 - acc: 0.0066 - val_loss: 11.7218 - val_acc: 0.0100\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7305 - acc: 0.0094 - val_loss: 11.7334 - val_acc: 0.0089\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.6417 - acc: 0.0083 - val_loss: 11.7308 - val_acc: 0.0107\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7742 - acc: 0.0103 - val_loss: 11.7281 - val_acc: 0.0113\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7741 - acc: 0.0097 - val_loss: 11.8311 - val_acc: 0.0111\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7163 - acc: 0.0091 - val_loss: 11.7146 - val_acc: 0.0129\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7878 - acc: 0.0099 - val_loss: 11.7186 - val_acc: 0.0146\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7238 - acc: 0.0114 - val_loss: 11.7092 - val_acc: 0.0117\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7520 - acc: 0.0097 - val_loss: 11.7026 - val_acc: 0.0126\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.6970 - acc: 0.0109 - val_loss: 11.7201 - val_acc: 0.0129\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7189 - acc: 0.0068 - val_loss: 11.7154 - val_acc: 0.0119\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 11.7621 - acc: 0.0094 - val_loss: 11.7407 - val_acc: 0.0090\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 11.7371 - acc: 0.0097 - val_loss: 12.0563 - val_acc: 0.0114\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 11.7766 - acc: 0.0093 - val_loss: 11.7118 - val_acc: 0.0110\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 11.8107 - acc: 0.0053 - val_loss: 11.7294 - val_acc: 0.0099\n",
            "Trainingelu Activation function\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 19s 38ms/step - loss: 7.0155 - acc: 0.0051 - val_loss: 5.7119 - val_acc: 0.0050\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.7651 - acc: 0.0062 - val_loss: 5.5882 - val_acc: 0.0050\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5862 - acc: 0.0050 - val_loss: 5.5297 - val_acc: 0.0047\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5700 - acc: 0.0071 - val_loss: 5.5237 - val_acc: 0.0060\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5649 - acc: 0.0094 - val_loss: 5.5027 - val_acc: 0.0123\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 10.4062 - acc: 0.0066 - val_loss: 5.4902 - val_acc: 0.0123\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5805 - acc: 0.0118 - val_loss: 5.4471 - val_acc: 0.0153\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5354 - acc: 0.0099 - val_loss: 5.4498 - val_acc: 0.0100\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5217 - acc: 0.0124 - val_loss: 5.4045 - val_acc: 0.0159\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5113 - acc: 0.0132 - val_loss: 5.4117 - val_acc: 0.0234\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.5350 - acc: 0.0149 - val_loss: 5.4444 - val_acc: 0.0205\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 5.5265 - acc: 0.0173 - val_loss: 5.4036 - val_acc: 0.0158\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5251 - acc: 0.0175 - val_loss: 5.3892 - val_acc: 0.0252\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5174 - acc: 0.0193 - val_loss: 5.3541 - val_acc: 0.0306\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 6.1228 - acc: 0.0189 - val_loss: 13.6268 - val_acc: 0.0089\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 6.8502 - acc: 0.0126 - val_loss: 5.4416 - val_acc: 0.0208\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.7797 - acc: 0.0123 - val_loss: 5.6369 - val_acc: 0.0236\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 6.7634 - acc: 0.0118 - val_loss: 5.4696 - val_acc: 0.0128\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.6181 - acc: 0.0131 - val_loss: 5.4392 - val_acc: 0.0123\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5088 - acc: 0.0186 - val_loss: 5.3841 - val_acc: 0.0257\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5129 - acc: 0.0192 - val_loss: 5.4114 - val_acc: 0.0249\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5486 - acc: 0.0200 - val_loss: 5.3480 - val_acc: 0.0255\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5174 - acc: 0.0206 - val_loss: 5.4738 - val_acc: 0.0169\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5178 - acc: 0.0229 - val_loss: 5.4146 - val_acc: 0.0194\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.6976 - acc: 0.0227 - val_loss: 5.4946 - val_acc: 0.0268\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5923 - acc: 0.0214 - val_loss: 5.3468 - val_acc: 0.0307\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5737 - acc: 0.0281 - val_loss: 5.5310 - val_acc: 0.0309\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5819 - acc: 0.0249 - val_loss: 5.3849 - val_acc: 0.0365\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.6803 - acc: 0.0243 - val_loss: 5.6817 - val_acc: 0.0294\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.7014 - acc: 0.0273 - val_loss: 5.4840 - val_acc: 0.0297\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.5781 - acc: 0.0262 - val_loss: 5.3323 - val_acc: 0.0359\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 6.3099 - acc: 0.0156 - val_loss: 5.3493 - val_acc: 0.0249\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.4221 - acc: 0.0252 - val_loss: 5.2035 - val_acc: 0.0346\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 5.3752 - acc: 0.0262 - val_loss: 5.1929 - val_acc: 0.0383\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.4122 - acc: 0.0304 - val_loss: 5.2029 - val_acc: 0.0415\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 5.4141 - acc: 0.0307 - val_loss: 5.2111 - val_acc: 0.0383\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.8428 - acc: 0.0275 - val_loss: 5.6718 - val_acc: 0.0239\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.6931 - acc: 0.0198 - val_loss: 5.8150 - val_acc: 0.0179\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.4729 - acc: 0.0165 - val_loss: 5.2104 - val_acc: 0.0257\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.3970 - acc: 0.0214 - val_loss: 5.1601 - val_acc: 0.0357\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.3076 - acc: 0.0225 - val_loss: 5.1023 - val_acc: 0.0379\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 5.3434 - acc: 0.0262 - val_loss: 5.2189 - val_acc: 0.0270\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.6003 - acc: 0.0244 - val_loss: 5.8873 - val_acc: 0.0248\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.8237 - acc: 0.0236 - val_loss: 5.3957 - val_acc: 0.0377\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.4107 - acc: 0.0301 - val_loss: 5.2606 - val_acc: 0.0363\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 6.3199 - acc: 0.0233 - val_loss: 6.2604 - val_acc: 0.0159\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.8438 - acc: 0.0184 - val_loss: 5.2969 - val_acc: 0.0316\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.6426 - acc: 0.0233 - val_loss: 5.4018 - val_acc: 0.0231\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.3321 - acc: 0.0237 - val_loss: 5.1305 - val_acc: 0.0414\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 5.3724 - acc: 0.0265 - val_loss: 5.0860 - val_acc: 0.0391\n",
            "Trainingtanh Activation function\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 19s 39ms/step - loss: 8.4497 - acc: 0.0051 - val_loss: 8.0566 - val_acc: 0.0050\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.0422 - acc: 0.0053 - val_loss: 8.0502 - val_acc: 0.0042\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.0874 - acc: 0.0056 - val_loss: 8.0566 - val_acc: 0.0050\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.1406 - acc: 0.0054 - val_loss: 8.0574 - val_acc: 0.0060\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.1491 - acc: 0.0048 - val_loss: 8.0590 - val_acc: 0.0050\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 8.0433 - acc: 0.0045 - val_loss: 8.0599 - val_acc: 0.0052\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 8.0111 - acc: 0.0053 - val_loss: 8.0719 - val_acc: 0.0052\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.0623 - acc: 0.0056 - val_loss: 8.0590 - val_acc: 0.0050\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.1680 - acc: 0.0052 - val_loss: 8.0599 - val_acc: 0.0050\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.1509 - acc: 0.0056 - val_loss: 8.0590 - val_acc: 0.0065\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 8.1259 - acc: 0.0051 - val_loss: 8.0590 - val_acc: 0.0043\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 7.9041 - acc: 0.0057 - val_loss: 8.1461 - val_acc: 0.0051\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 7.9695 - acc: 0.0051 - val_loss: 8.0623 - val_acc: 0.0050\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 8.1134 - acc: 0.0048 - val_loss: 8.0590 - val_acc: 0.0050\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 8.0150 - acc: 0.0047 - val_loss: 8.0590 - val_acc: 0.0050\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 8.1206 - acc: 0.0044 - val_loss: 8.0590 - val_acc: 0.0050\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 8.0784 - acc: 0.0057 - val_loss: 8.0574 - val_acc: 0.0050\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 2.8227 - acc: 0.0051 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0050 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0052 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 1.1921e-07 - acc: 0.0057 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0049 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0050 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0051 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0044 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0050 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0047 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0058 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 1.1921e-07 - acc: 0.0042 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0053 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0053 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 1.1921e-07 - acc: 0.0044 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0047 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0059 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0047 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0046 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0054 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0049 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0047 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0053 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1921e-07 - acc: 0.0055 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 1.1921e-07 - acc: 0.0052 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0047 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0047 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 1.1921e-07 - acc: 0.0048 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0049 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0049 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0050 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0051 - val_loss: 1.1921e-07 - val_acc: 0.0050\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1921e-07 - acc: 0.0051 - val_loss: 1.1921e-07 - val_acc: 0.0050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_v6XQshDoe9c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   elu activation resulted in the best performance\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8qk4Z2lQok8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model and modify the batch size"
      ]
    },
    {
      "metadata": {
        "id": "qA8DfcwxosNh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Btach size [32, 64, 128]"
      ]
    },
    {
      "metadata": {
        "id": "wZjW6dEikeqE",
        "colab_type": "code",
        "outputId": "a27d6d17-3a19-42ca-895b-25e088aad858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9156
        }
      },
      "cell_type": "code",
      "source": [
        "batchsize = [32, 64, 128, 512]\n",
        "\n",
        "Accuracy_list = []\n",
        "\n",
        "num_classes = 200\n",
        "num_epochs = 50\n",
        "num_predictions = 20\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "for bat in batchsize:\n",
        "  print('Training' + str(bat) + ' Batchsize')\n",
        "  \n",
        "  logs = \"logs/batch_size/\"+str(bat)     \n",
        "  model_name = str(bat) + '_Keras_Batchsize_TinyImagenet200.h5'\n",
        "  \n",
        "  batch_size = bat\n",
        "  \n",
        "  data_augmentation = True\n",
        "  \n",
        "  start = time()\n",
        "    \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.summary()\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adamax(), metrics=['accuracy'])\n",
        "\n",
        "  start = time()\n",
        "    \n",
        "  if not data_augmentation:\n",
        "    print('Not using data augmentation.') \n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "\t\t\t        epochs=num_epochs,\n",
        "\t\t\t        validation_data=(x_val, y_val),\n",
        "\t\t\t        shuffle=True)\n",
        "  \n",
        "  else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                             batch_size=batch_size),\n",
        "                                epochs=num_epochs,\n",
        "                                steps_per_epoch=500,\n",
        "                                workers = 4,\n",
        "                                validation_data=(x_val, y_val)\n",
        "                                )\n",
        "        \n",
        "        \n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "    end = time()\n",
        "    print('Time taken to run:', str(end-start))\n",
        "\n",
        "    print('Validation loss:', scores[0])\n",
        "    print('Validation accuracy:', scores[1])\n",
        "    \n",
        "    Accuracy_list.append([str(bat),scores[1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training32 Batchsize\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 5.1145 - acc: 0.0233 - val_loss: 4.8457 - val_acc: 0.0457\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.8862 - acc: 0.0421 - val_loss: 4.6972 - val_acc: 0.0639\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.7221 - acc: 0.0590 - val_loss: 4.5101 - val_acc: 0.0876\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.6272 - acc: 0.0713 - val_loss: 4.4337 - val_acc: 0.0956\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.5085 - acc: 0.0833 - val_loss: 4.3616 - val_acc: 0.1033\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.4100 - acc: 0.0949 - val_loss: 4.2969 - val_acc: 0.1086\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.3484 - acc: 0.1083 - val_loss: 4.2789 - val_acc: 0.1168\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.3493 - acc: 0.1026 - val_loss: 4.3074 - val_acc: 0.1182\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.3049 - acc: 0.1087 - val_loss: 4.3414 - val_acc: 0.1104\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.2719 - acc: 0.1154 - val_loss: 4.1433 - val_acc: 0.1309\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.2012 - acc: 0.1269 - val_loss: 4.1249 - val_acc: 0.1369\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1799 - acc: 0.1271 - val_loss: 4.1299 - val_acc: 0.1359\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1809 - acc: 0.1309 - val_loss: 4.1289 - val_acc: 0.1376\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1668 - acc: 0.1260 - val_loss: 3.9961 - val_acc: 0.1560\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1629 - acc: 0.1281 - val_loss: 3.9642 - val_acc: 0.1584\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0963 - acc: 0.1393 - val_loss: 4.0206 - val_acc: 0.1557\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1043 - acc: 0.1435 - val_loss: 4.1481 - val_acc: 0.1416\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.1077 - acc: 0.1376 - val_loss: 4.0144 - val_acc: 0.1548\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 4.0776 - acc: 0.1424 - val_loss: 4.1632 - val_acc: 0.1427\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0678 - acc: 0.1500 - val_loss: 4.0860 - val_acc: 0.1469\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0349 - acc: 0.1456 - val_loss: 4.1001 - val_acc: 0.1480\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0365 - acc: 0.1509 - val_loss: 4.0537 - val_acc: 0.1524\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0386 - acc: 0.1507 - val_loss: 3.9964 - val_acc: 0.1628\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0483 - acc: 0.1481 - val_loss: 4.0501 - val_acc: 0.1545\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0324 - acc: 0.1532 - val_loss: 3.9807 - val_acc: 0.1600\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9972 - acc: 0.1559 - val_loss: 3.9753 - val_acc: 0.1603\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0211 - acc: 0.1515 - val_loss: 3.9803 - val_acc: 0.1640\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9647 - acc: 0.1628 - val_loss: 3.8475 - val_acc: 0.1814\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9929 - acc: 0.1551 - val_loss: 3.9453 - val_acc: 0.1674\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9898 - acc: 0.1543 - val_loss: 3.8493 - val_acc: 0.1812\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9433 - acc: 0.1649 - val_loss: 3.9250 - val_acc: 0.1727\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9838 - acc: 0.1562 - val_loss: 3.8628 - val_acc: 0.1788\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9724 - acc: 0.1599 - val_loss: 3.8961 - val_acc: 0.1752\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9344 - acc: 0.1623 - val_loss: 3.9291 - val_acc: 0.1724\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.9563 - acc: 0.1589 - val_loss: 3.8673 - val_acc: 0.1792\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.9002 - acc: 0.1645 - val_loss: 3.8813 - val_acc: 0.1775\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9331 - acc: 0.1649 - val_loss: 3.9012 - val_acc: 0.1761\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9296 - acc: 0.1659 - val_loss: 3.8177 - val_acc: 0.1853\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.9305 - acc: 0.1681 - val_loss: 3.7957 - val_acc: 0.1876\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9251 - acc: 0.1637 - val_loss: 4.0408 - val_acc: 0.1582\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8770 - acc: 0.1753 - val_loss: 3.8241 - val_acc: 0.1853\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8986 - acc: 0.1690 - val_loss: 3.8240 - val_acc: 0.1852\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9209 - acc: 0.1679 - val_loss: 3.8834 - val_acc: 0.1809\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.9125 - acc: 0.1704 - val_loss: 3.9043 - val_acc: 0.1785\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8851 - acc: 0.1686 - val_loss: 3.8560 - val_acc: 0.1779\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8707 - acc: 0.1764 - val_loss: 3.7769 - val_acc: 0.1878\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8658 - acc: 0.1731 - val_loss: 3.7894 - val_acc: 0.1925\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8831 - acc: 0.1730 - val_loss: 3.8633 - val_acc: 0.1833\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 3.8800 - acc: 0.1661 - val_loss: 3.8575 - val_acc: 0.1868\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.8799 - acc: 0.1731 - val_loss: 4.0157 - val_acc: 0.1660\n",
            "Training64 Batchsize\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 28s 57ms/step - loss: 5.0116 - acc: 0.0318 - val_loss: 4.7691 - val_acc: 0.0587\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 4.6906 - acc: 0.0653 - val_loss: 4.5156 - val_acc: 0.0864\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 4.4977 - acc: 0.0883 - val_loss: 4.3869 - val_acc: 0.1051\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 26s 53ms/step - loss: 4.3422 - acc: 0.1052 - val_loss: 4.3459 - val_acc: 0.1074\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 4.2823 - acc: 0.1123 - val_loss: 4.1763 - val_acc: 0.1298\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 4.1911 - acc: 0.1233 - val_loss: 4.1400 - val_acc: 0.1414\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 4.1715 - acc: 0.1286 - val_loss: 4.0353 - val_acc: 0.1517\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 4.1141 - acc: 0.1382 - val_loss: 4.0334 - val_acc: 0.1492\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 4.0791 - acc: 0.1396 - val_loss: 4.0562 - val_acc: 0.1484\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 4.0699 - acc: 0.1430 - val_loss: 4.0254 - val_acc: 0.1561\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 4.0170 - acc: 0.1529 - val_loss: 4.0309 - val_acc: 0.1583\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 4.0208 - acc: 0.1512 - val_loss: 3.9923 - val_acc: 0.1642\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.9795 - acc: 0.1591 - val_loss: 3.8396 - val_acc: 0.1795\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 3.9640 - acc: 0.1577 - val_loss: 3.9578 - val_acc: 0.1666\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.9600 - acc: 0.1589 - val_loss: 3.8963 - val_acc: 0.1757\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.9069 - acc: 0.1695 - val_loss: 3.8766 - val_acc: 0.1772\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.9369 - acc: 0.1631 - val_loss: 3.8472 - val_acc: 0.1806\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8949 - acc: 0.1719 - val_loss: 3.9108 - val_acc: 0.1726\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 3.8941 - acc: 0.1702 - val_loss: 3.7930 - val_acc: 0.1906\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8961 - acc: 0.1734 - val_loss: 3.7097 - val_acc: 0.1967\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8572 - acc: 0.1736 - val_loss: 3.8114 - val_acc: 0.1840\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8759 - acc: 0.1740 - val_loss: 3.7918 - val_acc: 0.1908\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8488 - acc: 0.1739 - val_loss: 3.7760 - val_acc: 0.1945\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8245 - acc: 0.1817 - val_loss: 3.7341 - val_acc: 0.1974\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 3.8477 - acc: 0.1767 - val_loss: 3.7268 - val_acc: 0.1976\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8033 - acc: 0.1840 - val_loss: 3.7863 - val_acc: 0.1927\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8104 - acc: 0.1846 - val_loss: 3.7529 - val_acc: 0.1953\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.8080 - acc: 0.1834 - val_loss: 3.6496 - val_acc: 0.2107\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7823 - acc: 0.1871 - val_loss: 3.7858 - val_acc: 0.1888\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 3.7925 - acc: 0.1853 - val_loss: 3.8091 - val_acc: 0.1953\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7622 - acc: 0.1917 - val_loss: 3.8815 - val_acc: 0.1850\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7755 - acc: 0.1897 - val_loss: 3.6172 - val_acc: 0.2140\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7381 - acc: 0.1909 - val_loss: 3.7652 - val_acc: 0.1963\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7361 - acc: 0.1920 - val_loss: 3.6553 - val_acc: 0.2061\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7495 - acc: 0.1909 - val_loss: 3.7418 - val_acc: 0.2066\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7151 - acc: 0.1977 - val_loss: 3.6254 - val_acc: 0.2152\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7289 - acc: 0.1901 - val_loss: 3.6203 - val_acc: 0.2130\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 3.7072 - acc: 0.2002 - val_loss: 3.6232 - val_acc: 0.2147\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 27s 53ms/step - loss: 3.7094 - acc: 0.1970 - val_loss: 3.6335 - val_acc: 0.2131\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 26s 53ms/step - loss: 3.7230 - acc: 0.1942 - val_loss: 3.6694 - val_acc: 0.2112\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 3.6802 - acc: 0.2010 - val_loss: 3.6526 - val_acc: 0.2122\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 26s 53ms/step - loss: 3.7124 - acc: 0.1961 - val_loss: 3.5699 - val_acc: 0.2262\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 26s 53ms/step - loss: 3.6726 - acc: 0.2020 - val_loss: 3.6226 - val_acc: 0.2205\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 26s 53ms/step - loss: 3.6641 - acc: 0.2053 - val_loss: 3.5261 - val_acc: 0.2348\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 3.6918 - acc: 0.1984 - val_loss: 3.6368 - val_acc: 0.2171\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 3.6408 - acc: 0.2067 - val_loss: 3.5843 - val_acc: 0.2241\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 26s 53ms/step - loss: 3.6579 - acc: 0.2021 - val_loss: 3.5587 - val_acc: 0.2233\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 3.6510 - acc: 0.2067 - val_loss: 3.5505 - val_acc: 0.2275\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 3.6404 - acc: 0.2068 - val_loss: 3.5895 - val_acc: 0.2212\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 26s 51ms/step - loss: 3.6650 - acc: 0.2062 - val_loss: 3.5731 - val_acc: 0.2273\n",
            "Training128 Batchsize\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 45s 91ms/step - loss: 4.8854 - acc: 0.0427 - val_loss: 4.5519 - val_acc: 0.0805\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 43s 87ms/step - loss: 4.4879 - acc: 0.0875 - val_loss: 4.2997 - val_acc: 0.1210\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 4.2840 - acc: 0.1134 - val_loss: 4.1961 - val_acc: 0.1320\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 4.1808 - acc: 0.1286 - val_loss: 4.0724 - val_acc: 0.1464\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 4.1025 - acc: 0.1389 - val_loss: 3.9441 - val_acc: 0.1615\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 43s 86ms/step - loss: 4.0306 - acc: 0.1529 - val_loss: 3.9325 - val_acc: 0.1623\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 3.9802 - acc: 0.1588 - val_loss: 3.9496 - val_acc: 0.1668\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 3.9463 - acc: 0.1613 - val_loss: 3.9135 - val_acc: 0.1759\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 3.9197 - acc: 0.1681 - val_loss: 3.9742 - val_acc: 0.1726\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 3.8849 - acc: 0.1750 - val_loss: 3.8131 - val_acc: 0.1861\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.8539 - acc: 0.1772 - val_loss: 3.8089 - val_acc: 0.1911\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.8353 - acc: 0.1806 - val_loss: 3.7416 - val_acc: 0.1986\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 3.8050 - acc: 0.1829 - val_loss: 3.6802 - val_acc: 0.2047\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.7886 - acc: 0.1886 - val_loss: 3.6672 - val_acc: 0.2107\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.7695 - acc: 0.1875 - val_loss: 3.6553 - val_acc: 0.2144\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.7483 - acc: 0.1907 - val_loss: 3.6386 - val_acc: 0.2131\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.7295 - acc: 0.1980 - val_loss: 3.5780 - val_acc: 0.2216\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3.7190 - acc: 0.1976 - val_loss: 3.5384 - val_acc: 0.2303\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3.7026 - acc: 0.2004 - val_loss: 3.5866 - val_acc: 0.2196\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.6977 - acc: 0.2004 - val_loss: 3.5541 - val_acc: 0.2264\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 42s 84ms/step - loss: 3.6668 - acc: 0.2051 - val_loss: 3.5707 - val_acc: 0.2256\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 40s 81ms/step - loss: 3.6558 - acc: 0.2069 - val_loss: 3.5659 - val_acc: 0.2245\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 41s 81ms/step - loss: 3.6395 - acc: 0.2057 - val_loss: 3.5156 - val_acc: 0.2316\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 41s 81ms/step - loss: 3.6318 - acc: 0.2085 - val_loss: 3.4745 - val_acc: 0.2384\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 41s 82ms/step - loss: 3.6238 - acc: 0.2116 - val_loss: 3.4774 - val_acc: 0.2371\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 41s 81ms/step - loss: 3.5944 - acc: 0.2141 - val_loss: 3.4486 - val_acc: 0.2423\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 40s 81ms/step - loss: 3.5983 - acc: 0.2132 - val_loss: 3.4354 - val_acc: 0.2457\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3.5844 - acc: 0.2160 - val_loss: 3.4458 - val_acc: 0.2485\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.5718 - acc: 0.2193 - val_loss: 3.4415 - val_acc: 0.2472\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.5733 - acc: 0.2168 - val_loss: 3.3987 - val_acc: 0.2541\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.5380 - acc: 0.2235 - val_loss: 3.3975 - val_acc: 0.2520\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 40s 81ms/step - loss: 3.5429 - acc: 0.2223 - val_loss: 3.4519 - val_acc: 0.2449\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 41s 81ms/step - loss: 3.5155 - acc: 0.2251 - val_loss: 3.3441 - val_acc: 0.2646\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.5238 - acc: 0.2260 - val_loss: 3.3499 - val_acc: 0.2661\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.5139 - acc: 0.2277 - val_loss: 3.3383 - val_acc: 0.2649\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3.4893 - acc: 0.2313 - val_loss: 3.3810 - val_acc: 0.2595\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 39s 79ms/step - loss: 3.4952 - acc: 0.2285 - val_loss: 3.3305 - val_acc: 0.2684\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.4801 - acc: 0.2322 - val_loss: 3.3276 - val_acc: 0.2658\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.4737 - acc: 0.2324 - val_loss: 3.3346 - val_acc: 0.2703\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.4752 - acc: 0.2332 - val_loss: 3.3406 - val_acc: 0.2650\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 40s 80ms/step - loss: 3.4580 - acc: 0.2361 - val_loss: 3.3223 - val_acc: 0.2699\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 39s 78ms/step - loss: 3.4603 - acc: 0.2369 - val_loss: 3.2973 - val_acc: 0.2717\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 39s 78ms/step - loss: 3.4373 - acc: 0.2377 - val_loss: 3.2908 - val_acc: 0.2728\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 42s 83ms/step - loss: 3.4481 - acc: 0.2387 - val_loss: 3.2549 - val_acc: 0.2780\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 40s 79ms/step - loss: 3.4327 - acc: 0.2389 - val_loss: 3.2924 - val_acc: 0.2752\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 39s 78ms/step - loss: 3.4244 - acc: 0.2384 - val_loss: 3.3547 - val_acc: 0.2691\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 40s 81ms/step - loss: 3.4246 - acc: 0.2396 - val_loss: 3.2785 - val_acc: 0.2795\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 41s 83ms/step - loss: 3.4200 - acc: 0.2401 - val_loss: 3.2681 - val_acc: 0.2806\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 42s 85ms/step - loss: 3.3994 - acc: 0.2441 - val_loss: 3.2493 - val_acc: 0.2834\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 43s 85ms/step - loss: 3.4113 - acc: 0.2408 - val_loss: 3.3104 - val_acc: 0.2762\n",
            "Training512 Batchsize\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 153s 306ms/step - loss: 4.6175 - acc: 0.0758 - val_loss: 4.1953 - val_acc: 0.1323\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 150s 300ms/step - loss: 4.1358 - acc: 0.1355 - val_loss: 3.9431 - val_acc: 0.1638\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 151s 303ms/step - loss: 3.9502 - acc: 0.1624 - val_loss: 3.8351 - val_acc: 0.1847\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 151s 303ms/step - loss: 3.8391 - acc: 0.1789 - val_loss: 3.7023 - val_acc: 0.2011\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 149s 298ms/step - loss: 3.7640 - acc: 0.1912 - val_loss: 3.5823 - val_acc: 0.2205\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 146s 293ms/step - loss: 3.6991 - acc: 0.1998 - val_loss: 3.5201 - val_acc: 0.2363\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 146s 292ms/step - loss: 3.6374 - acc: 0.2099 - val_loss: 3.4252 - val_acc: 0.2453\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 144s 287ms/step - loss: 3.5884 - acc: 0.2163 - val_loss: 3.3844 - val_acc: 0.2545\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 143s 286ms/step - loss: 3.5417 - acc: 0.2239 - val_loss: 3.3320 - val_acc: 0.2591\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 142s 285ms/step - loss: 3.4988 - acc: 0.2300 - val_loss: 3.2895 - val_acc: 0.2712\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 143s 286ms/step - loss: 3.4502 - acc: 0.2389 - val_loss: 3.2870 - val_acc: 0.2713\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 140s 279ms/step - loss: 3.4158 - acc: 0.2423 - val_loss: 3.2553 - val_acc: 0.2811\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 140s 281ms/step - loss: 3.3894 - acc: 0.2474 - val_loss: 3.2466 - val_acc: 0.2802\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 139s 278ms/step - loss: 3.3494 - acc: 0.2527 - val_loss: 3.1450 - val_acc: 0.2964\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 137s 275ms/step - loss: 3.3208 - acc: 0.2572 - val_loss: 3.1875 - val_acc: 0.2944\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 138s 275ms/step - loss: 3.2955 - acc: 0.2628 - val_loss: 3.1021 - val_acc: 0.3043\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 136s 271ms/step - loss: 3.2699 - acc: 0.2656 - val_loss: 3.1381 - val_acc: 0.3000\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 138s 276ms/step - loss: 3.2423 - acc: 0.2689 - val_loss: 3.0755 - val_acc: 0.3085\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 135s 270ms/step - loss: 3.2213 - acc: 0.2734 - val_loss: 3.0759 - val_acc: 0.3102\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 137s 274ms/step - loss: 3.2052 - acc: 0.2748 - val_loss: 3.0473 - val_acc: 0.3171\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 3.1808 - acc: 0.2792 - val_loss: 3.0781 - val_acc: 0.3125\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 135s 270ms/step - loss: 3.1652 - acc: 0.2818 - val_loss: 3.0252 - val_acc: 0.3196\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 135s 271ms/step - loss: 3.1535 - acc: 0.2828 - val_loss: 3.0096 - val_acc: 0.3241\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 3.1379 - acc: 0.2865 - val_loss: 3.0158 - val_acc: 0.3234\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 135s 270ms/step - loss: 3.1181 - acc: 0.2878 - val_loss: 3.0177 - val_acc: 0.3212\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 133s 267ms/step - loss: 3.1083 - acc: 0.2896 - val_loss: 3.0342 - val_acc: 0.3187\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 135s 271ms/step - loss: 3.0973 - acc: 0.2928 - val_loss: 2.9576 - val_acc: 0.3345\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 133s 265ms/step - loss: 3.0767 - acc: 0.2953 - val_loss: 3.0044 - val_acc: 0.3238\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 3.0647 - acc: 0.2981 - val_loss: 2.9732 - val_acc: 0.3303\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 3.0614 - acc: 0.2985 - val_loss: 2.9259 - val_acc: 0.3381\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 132s 264ms/step - loss: 3.0458 - acc: 0.3005 - val_loss: 2.9697 - val_acc: 0.3315\n",
            "Epoch 32/50\n",
            "404/500 [=======================>......] - ETA: 25s - loss: 3.0409 - acc: 0.3009Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6v3TsLAozYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Btach size [512]"
      ]
    },
    {
      "metadata": {
        "id": "Pq2hMYm9nRFs",
        "colab_type": "code",
        "outputId": "38490361-aba1-401c-8779-6f17b53910cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2624
        }
      },
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "\n",
        "batchsize = [512]\n",
        "Accuracy_list = []\n",
        "\n",
        "num_classes = 200\n",
        "num_epochs = 50\n",
        "num_predictions = 20\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "for bat in batchsize:\n",
        "  print('Training' + str(bat) + ' Batchsize')\n",
        "  \n",
        "  logs = \"logs/batch_size/\"+str(bat)     \n",
        "  model_name = str(bat) + '_Keras_Batchsize_TinyImagenet200.h5'\n",
        "  \n",
        "  batch_size = bat\n",
        "  \n",
        "  data_augmentation = True\n",
        "  \n",
        "  start = time()\n",
        "    \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.summary()\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adamax(), metrics=['accuracy'])\n",
        "\n",
        "  start = time()\n",
        "    \n",
        "  if not data_augmentation:\n",
        "    print('Not using data augmentation.') \n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "\t\t\t        epochs=num_epochs,\n",
        "\t\t\t        validation_data=(x_val, y_val),\n",
        "\t\t\t        shuffle=True)\n",
        "  \n",
        "  else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                             batch_size=batch_size),\n",
        "                                epochs=num_epochs,\n",
        "                                steps_per_epoch=500,\n",
        "                                workers = 4,\n",
        "                                validation_data=(x_val, y_val)\n",
        "                                )\n",
        "        \n",
        "        \n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "    end = time()\n",
        "    print('Time taken to run:', str(end-start))\n",
        "\n",
        "    print('Validation loss:', scores[0])\n",
        "    print('Validation accuracy:', scores[1])\n",
        "    \n",
        "    Accuracy_list.append([str(bat),scores[1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training512 Batchsize\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 156s 312ms/step - loss: 4.5995 - acc: 0.0775 - val_loss: 4.2214 - val_acc: 0.1226\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 149s 298ms/step - loss: 4.1444 - acc: 0.1346 - val_loss: 3.9018 - val_acc: 0.1723\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 149s 298ms/step - loss: 3.9657 - acc: 0.1595 - val_loss: 3.7864 - val_acc: 0.1900\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 147s 294ms/step - loss: 3.8521 - acc: 0.1781 - val_loss: 3.6487 - val_acc: 0.2101\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 146s 292ms/step - loss: 3.7782 - acc: 0.1884 - val_loss: 3.5749 - val_acc: 0.2258\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 144s 288ms/step - loss: 3.7083 - acc: 0.1997 - val_loss: 3.5555 - val_acc: 0.2275\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 143s 286ms/step - loss: 3.6526 - acc: 0.2068 - val_loss: 3.4682 - val_acc: 0.2414\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 142s 285ms/step - loss: 3.6022 - acc: 0.2144 - val_loss: 3.4313 - val_acc: 0.2493\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 3.5627 - acc: 0.2204 - val_loss: 3.3586 - val_acc: 0.2610\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 140s 280ms/step - loss: 3.5195 - acc: 0.2285 - val_loss: 3.2770 - val_acc: 0.2736\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 147s 293ms/step - loss: 3.4784 - acc: 0.2338 - val_loss: 3.3111 - val_acc: 0.2683\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 148s 297ms/step - loss: 3.4408 - acc: 0.2381 - val_loss: 3.2659 - val_acc: 0.2787\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 138s 276ms/step - loss: 3.4078 - acc: 0.2447 - val_loss: 3.2467 - val_acc: 0.2776\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 139s 277ms/step - loss: 3.3720 - acc: 0.2512 - val_loss: 3.1612 - val_acc: 0.2935\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 139s 277ms/step - loss: 3.3435 - acc: 0.2550 - val_loss: 3.1917 - val_acc: 0.2929\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 153s 307ms/step - loss: 3.3161 - acc: 0.2588 - val_loss: 3.1139 - val_acc: 0.3033\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 144s 288ms/step - loss: 3.2887 - acc: 0.2640 - val_loss: 3.1093 - val_acc: 0.3060\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 138s 276ms/step - loss: 3.2661 - acc: 0.2657 - val_loss: 3.0949 - val_acc: 0.3136\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 138s 275ms/step - loss: 3.2366 - acc: 0.2714 - val_loss: 3.0945 - val_acc: 0.3090\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 138s 277ms/step - loss: 3.2202 - acc: 0.2737 - val_loss: 3.0949 - val_acc: 0.3124\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 138s 276ms/step - loss: 3.2019 - acc: 0.2766 - val_loss: 3.0712 - val_acc: 0.3138\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 137s 274ms/step - loss: 3.1813 - acc: 0.2791 - val_loss: 3.0420 - val_acc: 0.3207\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 137s 274ms/step - loss: 3.1606 - acc: 0.2825 - val_loss: 3.0527 - val_acc: 0.3170\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 137s 274ms/step - loss: 3.1469 - acc: 0.2841 - val_loss: 3.0037 - val_acc: 0.3271\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 137s 274ms/step - loss: 3.1342 - acc: 0.2870 - val_loss: 3.0475 - val_acc: 0.3214\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 135s 271ms/step - loss: 3.1220 - acc: 0.2886 - val_loss: 2.9553 - val_acc: 0.3348\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 133s 267ms/step - loss: 3.1079 - acc: 0.2904 - val_loss: 2.9666 - val_acc: 0.3342\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 135s 270ms/step - loss: 3.0913 - acc: 0.2937 - val_loss: 2.9780 - val_acc: 0.3326\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 3.0793 - acc: 0.2948 - val_loss: 2.9641 - val_acc: 0.3362\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 136s 272ms/step - loss: 3.0752 - acc: 0.2953 - val_loss: 2.9947 - val_acc: 0.3307\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 3.0591 - acc: 0.2982 - val_loss: 2.9616 - val_acc: 0.3336\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 135s 270ms/step - loss: 3.0519 - acc: 0.2994 - val_loss: 2.9758 - val_acc: 0.3360\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 3.0374 - acc: 0.3023 - val_loss: 2.9221 - val_acc: 0.3385\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 133s 266ms/step - loss: 3.0294 - acc: 0.3031 - val_loss: 2.9713 - val_acc: 0.3317\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 136s 272ms/step - loss: 3.0207 - acc: 0.3035 - val_loss: 2.9525 - val_acc: 0.3362\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 133s 267ms/step - loss: 3.0102 - acc: 0.3057 - val_loss: 2.9571 - val_acc: 0.3336\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 2.9997 - acc: 0.3068 - val_loss: 2.9313 - val_acc: 0.3377\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 2.9960 - acc: 0.3089 - val_loss: 2.9177 - val_acc: 0.3392\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 135s 269ms/step - loss: 2.9842 - acc: 0.3087 - val_loss: 2.9580 - val_acc: 0.3365\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 2.9804 - acc: 0.3102 - val_loss: 2.9231 - val_acc: 0.3414\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 135s 270ms/step - loss: 2.9710 - acc: 0.3119 - val_loss: 2.9202 - val_acc: 0.3408\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 2.9681 - acc: 0.3130 - val_loss: 2.8990 - val_acc: 0.3472\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 133s 267ms/step - loss: 2.9620 - acc: 0.3141 - val_loss: 2.9260 - val_acc: 0.3396\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 135s 271ms/step - loss: 2.9571 - acc: 0.3144 - val_loss: 2.8852 - val_acc: 0.3467\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 2.9498 - acc: 0.3147 - val_loss: 2.9051 - val_acc: 0.3433\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 135s 269ms/step - loss: 2.9389 - acc: 0.3164 - val_loss: 2.9088 - val_acc: 0.3427\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 134s 267ms/step - loss: 2.9308 - acc: 0.3192 - val_loss: 2.8952 - val_acc: 0.3495\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 2.9274 - acc: 0.3189 - val_loss: 2.8691 - val_acc: 0.3460\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 133s 266ms/step - loss: 2.9260 - acc: 0.3190 - val_loss: 2.8791 - val_acc: 0.3484\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 133s 266ms/step - loss: 2.9158 - acc: 0.3200 - val_loss: 2.9252 - val_acc: 0.3444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7BuQvECv94ou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hidden Layers & Neurons"
      ]
    },
    {
      "metadata": {
        "id": "WB84qRbI8XU7",
        "colab_type": "code",
        "outputId": "3ddd3ba5-4a04-45ed-f79c-5867f49039a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2826
        }
      },
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "\n",
        "batch_size = 512\n",
        "num_classes = 200\n",
        "num_epochs = 50\n",
        "num_predictions = 20\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "print('Training 3 layers')\n",
        "  \n",
        "logs = \"logs/3layers/\"     \n",
        "model_name = 'Keras_3layers_TinyImagenet200.h5'\n",
        "  \n",
        "data_augmentation = True\n",
        "start = time()\n",
        "  \n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', \n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(512, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "          \n",
        "model.summary()\n",
        "          \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adamax(), metrics=['accuracy'])\n",
        "\n",
        "start = time()\n",
        "    \n",
        "if not data_augmentation:\n",
        "  print('Not using data augmentation.') \n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "\t\t\t      validation_data=(x_val, y_val),\n",
        "\t\t\t      shuffle=True)\n",
        "  \n",
        "else:\n",
        "  print('Using real-time data augmentation.')\n",
        "  # This will do preprocessing and realtime data augmentation:\n",
        "  datagen = ImageDataGenerator(\n",
        "      featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "      samplewise_center=False,  # set each sample mean to 0\n",
        "      featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "      samplewise_std_normalization=False,  # divide each input by its std\n",
        "      zca_whitening=False,  # apply ZCA whitening\n",
        "      zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "      rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "      # randomly shift images horizontally (fraction of total width)\n",
        "      width_shift_range=0.1,\n",
        "      # randomly shift images vertically (fraction of total height)\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.,  # set range for random shear\n",
        "      zoom_range=0.,  # set range for random zoom\n",
        "      channel_shift_range=0.,  # set range for random channel shifts\n",
        "      # set mode for filling points outside the input boundaries\n",
        "      fill_mode='nearest',\n",
        "      cval=0.,  # value used for fill_mode = \"constant\"\n",
        "      horizontal_flip=True,  # randomly flip images\n",
        "      vertical_flip=False,  # randomly flip images\n",
        "      # set rescaling factor (applied before any other transformation\n",
        "      rescale=None,\n",
        "      # set function that will be applied on each input\n",
        "      preprocessing_function=None,\n",
        "      # image data format, either \"channels_first\" or \"channels_last\"\n",
        "      data_format=None,\n",
        "      # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "      validation_split=0.0)\n",
        "\n",
        "  # Compute quantities required for feature-wise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied).\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  # Fit the model on the batches generated by datagen.flow().\n",
        "  history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                           batch_size=batch_size),\n",
        "                              epochs=num_epochs,\n",
        "                              steps_per_epoch=500,\n",
        "                              workers = 4,\n",
        "                              validation_data=(x_val, y_val)\n",
        "                              )\n",
        "        \n",
        "        \n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)    \n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "  print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "  end = time()\n",
        "  print('Time taken to run:', str(end-start))\n",
        "\n",
        "  print('Validation loss:', scores[0])\n",
        "  print('Validation accuracy:', scores[1])\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 3 layers\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 6, 6, 512)         295424    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               205000    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 5,023,976\n",
            "Trainable params: 5,023,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 164s 329ms/step - loss: 4.5507 - acc: 0.0816 - val_loss: 4.1948 - val_acc: 0.1318\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 157s 315ms/step - loss: 4.0454 - acc: 0.1475 - val_loss: 3.9297 - val_acc: 0.1720\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 160s 319ms/step - loss: 3.8365 - acc: 0.1791 - val_loss: 3.7877 - val_acc: 0.1943\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 160s 321ms/step - loss: 3.7044 - acc: 0.1995 - val_loss: 3.5579 - val_acc: 0.2240\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 156s 312ms/step - loss: 3.6103 - acc: 0.2145 - val_loss: 3.5541 - val_acc: 0.2361\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 152s 305ms/step - loss: 3.5361 - acc: 0.2264 - val_loss: 3.4516 - val_acc: 0.2490\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 150s 301ms/step - loss: 3.4610 - acc: 0.2380 - val_loss: 3.3725 - val_acc: 0.2630\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 151s 302ms/step - loss: 3.4102 - acc: 0.2463 - val_loss: 3.3175 - val_acc: 0.2765\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 154s 308ms/step - loss: 3.3598 - acc: 0.2542 - val_loss: 3.1980 - val_acc: 0.2885\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 155s 310ms/step - loss: 3.3093 - acc: 0.2619 - val_loss: 3.2066 - val_acc: 0.2919\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 152s 304ms/step - loss: 3.2690 - acc: 0.2679 - val_loss: 3.1432 - val_acc: 0.3025\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 152s 304ms/step - loss: 3.2232 - acc: 0.2749 - val_loss: 3.1314 - val_acc: 0.3035\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 155s 311ms/step - loss: 3.1825 - acc: 0.2808 - val_loss: 3.1960 - val_acc: 0.2979\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 162s 324ms/step - loss: 3.1430 - acc: 0.2879 - val_loss: 3.0825 - val_acc: 0.3138\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 162s 324ms/step - loss: 3.1050 - acc: 0.2949 - val_loss: 3.1245 - val_acc: 0.3080\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 162s 323ms/step - loss: 3.0734 - acc: 0.2999 - val_loss: 3.0680 - val_acc: 0.3215\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 162s 323ms/step - loss: 3.0402 - acc: 0.3049 - val_loss: 3.0352 - val_acc: 0.3281\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 161s 323ms/step - loss: 3.0125 - acc: 0.3093 - val_loss: 3.1028 - val_acc: 0.3194\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 161s 322ms/step - loss: 2.9816 - acc: 0.3150 - val_loss: 2.9841 - val_acc: 0.3397\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 164s 328ms/step - loss: 2.9506 - acc: 0.3198 - val_loss: 2.9781 - val_acc: 0.3382\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 164s 328ms/step - loss: 2.9292 - acc: 0.3226 - val_loss: 2.9570 - val_acc: 0.3419\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 163s 325ms/step - loss: 2.9041 - acc: 0.3258 - val_loss: 3.0047 - val_acc: 0.3336\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 161s 322ms/step - loss: 2.8796 - acc: 0.3309 - val_loss: 3.0011 - val_acc: 0.3416\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 162s 325ms/step - loss: 2.8579 - acc: 0.3342 - val_loss: 2.9771 - val_acc: 0.3419\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.8407 - acc: 0.3374 - val_loss: 2.9212 - val_acc: 0.3485\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 164s 327ms/step - loss: 2.8235 - acc: 0.3395 - val_loss: 2.9371 - val_acc: 0.3475\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.8061 - acc: 0.3426 - val_loss: 2.9286 - val_acc: 0.3514\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 162s 325ms/step - loss: 2.7821 - acc: 0.3477 - val_loss: 3.0098 - val_acc: 0.3404\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 164s 328ms/step - loss: 2.7610 - acc: 0.3511 - val_loss: 2.9225 - val_acc: 0.3525\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.7479 - acc: 0.3528 - val_loss: 2.9898 - val_acc: 0.3443\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.7343 - acc: 0.3556 - val_loss: 2.9207 - val_acc: 0.3531\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.7216 - acc: 0.3568 - val_loss: 2.8999 - val_acc: 0.3528\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.6991 - acc: 0.3609 - val_loss: 2.9516 - val_acc: 0.3483\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.6808 - acc: 0.3646 - val_loss: 2.9901 - val_acc: 0.3479\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.6748 - acc: 0.3650 - val_loss: 2.9501 - val_acc: 0.3547\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 161s 323ms/step - loss: 2.6641 - acc: 0.3677 - val_loss: 3.0127 - val_acc: 0.3459\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.6458 - acc: 0.3691 - val_loss: 2.9745 - val_acc: 0.3507\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 163s 325ms/step - loss: 2.6373 - acc: 0.3710 - val_loss: 2.9910 - val_acc: 0.3456\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.6204 - acc: 0.3733 - val_loss: 2.9337 - val_acc: 0.3604\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 163s 327ms/step - loss: 2.6069 - acc: 0.3764 - val_loss: 2.9117 - val_acc: 0.3524\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 166s 333ms/step - loss: 2.5984 - acc: 0.3780 - val_loss: 2.9319 - val_acc: 0.3572\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 163s 327ms/step - loss: 2.5823 - acc: 0.3814 - val_loss: 2.9344 - val_acc: 0.3586\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 163s 326ms/step - loss: 2.5800 - acc: 0.3815 - val_loss: 2.9109 - val_acc: 0.3587\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 163s 325ms/step - loss: 2.5683 - acc: 0.3837 - val_loss: 2.9606 - val_acc: 0.3548\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 162s 324ms/step - loss: 2.5569 - acc: 0.3865 - val_loss: 2.9778 - val_acc: 0.3520\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 161s 323ms/step - loss: 2.5588 - acc: 0.3839 - val_loss: 2.9368 - val_acc: 0.3600\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 162s 323ms/step - loss: 2.5402 - acc: 0.3885 - val_loss: 2.9180 - val_acc: 0.3626\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 163s 325ms/step - loss: 2.5368 - acc: 0.3879 - val_loss: 2.9296 - val_acc: 0.3560\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 161s 322ms/step - loss: 2.5299 - acc: 0.3905 - val_loss: 2.9955 - val_acc: 0.3522\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 162s 324ms/step - loss: 2.5091 - acc: 0.3938 - val_loss: 2.9062 - val_acc: 0.3659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4btE2uCM86YR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "num_classes = 200\n",
        "num_epochs = 50\n",
        "num_predictions = 20\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "print('Training 4 layers')\n",
        "  \n",
        "logs = \"logs/4layers/\"     \n",
        "model_name = 'Keras_4layers_TinyImagenet200.h5'\n",
        "  \n",
        "data_augmentation = True\n",
        "start = time()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', \n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(512, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(1024, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(1024, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048)\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "          \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adamax(), metrics=['accuracy'])\n",
        "\n",
        "start = time()\n",
        "    \n",
        "if not data_augmentation:\n",
        "  print('Not using data augmentation.') \n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "\t\t\t      validation_data=(x_val, y_val),\n",
        "\t\t\t      shuffle=True)\n",
        "  \n",
        "else:\n",
        "  print('Using real-time data augmentation.')\n",
        "  # This will do preprocessing and realtime data augmentation:\n",
        "  datagen = ImageDataGenerator(\n",
        "      featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "      samplewise_center=False,  # set each sample mean to 0\n",
        "      featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "      samplewise_std_normalization=False,  # divide each input by its std\n",
        "      zca_whitening=False,  # apply ZCA whitening\n",
        "      zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "      rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "      # randomly shift images horizontally (fraction of total width)\n",
        "      width_shift_range=0.1,\n",
        "      # randomly shift images vertically (fraction of total height)\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.,  # set range for random shear\n",
        "      zoom_range=0.,  # set range for random zoom\n",
        "      channel_shift_range=0.,  # set range for random channel shifts\n",
        "      # set mode for filling points outside the input boundaries\n",
        "      fill_mode='nearest',\n",
        "      cval=0.,  # value used for fill_mode = \"constant\"\n",
        "      horizontal_flip=True,  # randomly flip images\n",
        "      vertical_flip=False,  # randomly flip images\n",
        "      # set rescaling factor (applied before any other transformation\n",
        "      rescale=None,\n",
        "      # set function that will be applied on each input\n",
        "      preprocessing_function=None,\n",
        "      # image data format, either \"channels_first\" or \"channels_last\"\n",
        "      data_format=None,\n",
        "      # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "      validation_split=0.0)\n",
        "\n",
        "  # Compute quantities required for feature-wise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied).\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  # Fit the model on the batches generated by datagen.flow().\n",
        "  history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                           batch_size=batch_size),\n",
        "                              epochs=num_epochs,\n",
        "                              steps_per_epoch=500,\n",
        "                              workers = 4,\n",
        "                              validation_data=(x_val, y_val)\n",
        "                              )\n",
        "        \n",
        "        \n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)    \n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "  print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "  end = time()\n",
        "  print('Time taken to run:', str(end-start))\n",
        "\n",
        "  print('Validation loss:', scores[0])\n",
        "  print('Validation accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egwTxjPFCD9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trying Batch Normalization"
      ]
    },
    {
      "metadata": {
        "id": "ln8baxCr7nD_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "batch_size = 512\n",
        "num_classes = 200\n",
        "num_epochs = 50\n",
        "num_predictions = 20\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "print('Training with batch normalization')\n",
        "  \n",
        "logs = \"logs/batch_normalization/\"     \n",
        "model_name = 'Keras_normalization_TinyImagenet200.h5'\n",
        "  \n",
        "data_augmentation = True\n",
        "start = time()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', \n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(512, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(1024, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(1024, (3, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048)\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "          \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adamax(), metrics=['accuracy'])\n",
        "\n",
        "start = time()\n",
        "    \n",
        "if not data_augmentation:\n",
        "  print('Not using data augmentation.') \n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "\t\t\t      validation_data=(x_val, y_val),\n",
        "\t\t\t      shuffle=True)\n",
        "  \n",
        "else:\n",
        "  print('Using real-time data augmentation.')\n",
        "  # This will do preprocessing and realtime data augmentation:\n",
        "  datagen = ImageDataGenerator(\n",
        "      featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "      samplewise_center=False,  # set each sample mean to 0\n",
        "      featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "      samplewise_std_normalization=False,  # divide each input by its std\n",
        "      zca_whitening=False,  # apply ZCA whitening\n",
        "      zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "      rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "      # randomly shift images horizontally (fraction of total width)\n",
        "      width_shift_range=0.1,\n",
        "      # randomly shift images vertically (fraction of total height)\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.,  # set range for random shear\n",
        "      zoom_range=0.,  # set range for random zoom\n",
        "      channel_shift_range=0.,  # set range for random channel shifts\n",
        "      # set mode for filling points outside the input boundaries\n",
        "      fill_mode='nearest',\n",
        "      cval=0.,  # value used for fill_mode = \"constant\"\n",
        "      horizontal_flip=True,  # randomly flip images\n",
        "      vertical_flip=False,  # randomly flip images\n",
        "      # set rescaling factor (applied before any other transformation\n",
        "      rescale=None,\n",
        "      # set function that will be applied on each input\n",
        "      preprocessing_function=None,\n",
        "      # image data format, either \"channels_first\" or \"channels_last\"\n",
        "      data_format=None,\n",
        "      # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "      validation_split=0.0)\n",
        "\n",
        "  # Compute quantities required for feature-wise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied).\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  # Fit the model on the batches generated by datagen.flow().\n",
        "  history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                           batch_size=batch_size),\n",
        "                              epochs=num_epochs,\n",
        "                              steps_per_epoch=500,\n",
        "                              workers = 4,\n",
        "                              validation_data=(x_val, y_val)\n",
        "                              )\n",
        "        \n",
        "        \n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)    \n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "  print('Saved trained model at %s ' % model_path)\n",
        "    \n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_val, y_val, verbose=1)\n",
        "\n",
        "  end = time()\n",
        "  print('Time taken to run:', str(end-start))\n",
        "\n",
        "  print('Validation loss:', scores[0])\n",
        "  print('Validation accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bTBkKBvs7z3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
