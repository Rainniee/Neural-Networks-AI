{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_CNN_New.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/Neural-Networks-AI/blob/master/Chatbot%20CNN%20Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qiK3oe2CBm5f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import hashlib\n",
        "import struct\n",
        "import subprocess\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNAVEMMzO_fT",
        "colab_type": "code",
        "outputId": "95602c35-eeb8-4b61-e06d-44b0788ff027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AeoDuZ9hMj-N",
        "colab_type": "code",
        "outputId": "ab09305e-889e-4d20-bb84-a47f5ee96ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5410
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip \"drive/My Drive/Assignment 4 Chatbot/finished_files.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/Assignment 4 Chatbot/finished_files.zip\n",
            "   creating: finished_files/\n",
            "  inflating: finished_files/test.bin  \n",
            "  inflating: finished_files/train.bin  \n",
            "  inflating: finished_files/val.bin  \n",
            "  inflating: finished_files/vocab    \n",
            "   creating: finished_files/chunked/\n",
            "  inflating: finished_files/chunked/train_127.bin  \n",
            "  inflating: finished_files/chunked/train_126.bin  \n",
            "  inflating: finished_files/chunked/train_125.bin  \n",
            "  inflating: finished_files/chunked/train_124.bin  \n",
            "  inflating: finished_files/chunked/train_123.bin  \n",
            "  inflating: finished_files/chunked/train_122.bin  \n",
            "  inflating: finished_files/chunked/train_121.bin  \n",
            "  inflating: finished_files/chunked/train_120.bin  \n",
            "  inflating: finished_files/chunked/train_119.bin  \n",
            "  inflating: finished_files/chunked/train_118.bin  \n",
            "  inflating: finished_files/chunked/train_117.bin  \n",
            "  inflating: finished_files/chunked/train_116.bin  \n",
            "  inflating: finished_files/chunked/train_115.bin  \n",
            "  inflating: finished_files/chunked/train_114.bin  \n",
            "  inflating: finished_files/chunked/train_113.bin  \n",
            "  inflating: finished_files/chunked/train_112.bin  \n",
            "  inflating: finished_files/chunked/train_111.bin  \n",
            "  inflating: finished_files/chunked/train_110.bin  \n",
            "  inflating: finished_files/chunked/train_109.bin  \n",
            "  inflating: finished_files/chunked/train_108.bin  \n",
            "  inflating: finished_files/chunked/train_107.bin  \n",
            "  inflating: finished_files/chunked/train_106.bin  \n",
            "  inflating: finished_files/chunked/train_105.bin  \n",
            "  inflating: finished_files/chunked/train_104.bin  \n",
            "  inflating: finished_files/chunked/train_103.bin  \n",
            "  inflating: finished_files/chunked/train_102.bin  \n",
            "  inflating: finished_files/chunked/train_101.bin  \n",
            "  inflating: finished_files/chunked/train_100.bin  \n",
            "  inflating: finished_files/chunked/train_099.bin  \n",
            "  inflating: finished_files/chunked/train_098.bin  \n",
            "  inflating: finished_files/chunked/train_097.bin  \n",
            "  inflating: finished_files/chunked/train_096.bin  \n",
            "  inflating: finished_files/chunked/train_095.bin  \n",
            "  inflating: finished_files/chunked/train_094.bin  \n",
            "  inflating: finished_files/chunked/train_093.bin  \n",
            "  inflating: finished_files/chunked/train_092.bin  \n",
            "  inflating: finished_files/chunked/train_091.bin  \n",
            "  inflating: finished_files/chunked/train_090.bin  \n",
            "  inflating: finished_files/chunked/train_089.bin  \n",
            "  inflating: finished_files/chunked/train_088.bin  \n",
            "  inflating: finished_files/chunked/train_087.bin  \n",
            "  inflating: finished_files/chunked/train_086.bin  \n",
            "  inflating: finished_files/chunked/train_085.bin  \n",
            "  inflating: finished_files/chunked/train_084.bin  \n",
            "  inflating: finished_files/chunked/train_083.bin  \n",
            "  inflating: finished_files/chunked/train_082.bin  \n",
            "  inflating: finished_files/chunked/train_081.bin  \n",
            "  inflating: finished_files/chunked/train_080.bin  \n",
            "  inflating: finished_files/chunked/train_079.bin  \n",
            "  inflating: finished_files/chunked/train_078.bin  \n",
            "  inflating: finished_files/chunked/train_077.bin  \n",
            "  inflating: finished_files/chunked/train_076.bin  \n",
            "  inflating: finished_files/chunked/train_075.bin  \n",
            "  inflating: finished_files/chunked/train_074.bin  \n",
            "  inflating: finished_files/chunked/train_073.bin  \n",
            "  inflating: finished_files/chunked/train_072.bin  \n",
            "  inflating: finished_files/chunked/train_071.bin  \n",
            "  inflating: finished_files/chunked/train_070.bin  \n",
            "  inflating: finished_files/chunked/train_069.bin  \n",
            "  inflating: finished_files/chunked/train_068.bin  \n",
            "  inflating: finished_files/chunked/train_067.bin  \n",
            "  inflating: finished_files/chunked/train_066.bin  \n",
            "  inflating: finished_files/chunked/train_065.bin  \n",
            "  inflating: finished_files/chunked/train_064.bin  \n",
            "  inflating: finished_files/chunked/train_063.bin  \n",
            "  inflating: finished_files/chunked/train_062.bin  \n",
            "  inflating: finished_files/chunked/train_061.bin  \n",
            "  inflating: finished_files/chunked/train_060.bin  \n",
            "  inflating: finished_files/chunked/train_059.bin  \n",
            "  inflating: finished_files/chunked/train_058.bin  \n",
            "  inflating: finished_files/chunked/train_057.bin  \n",
            "  inflating: finished_files/chunked/train_056.bin  \n",
            "  inflating: finished_files/chunked/train_055.bin  \n",
            "  inflating: finished_files/chunked/train_054.bin  \n",
            "  inflating: finished_files/chunked/train_053.bin  \n",
            "  inflating: finished_files/chunked/train_052.bin  \n",
            "  inflating: finished_files/chunked/train_051.bin  \n",
            "  inflating: finished_files/chunked/train_050.bin  \n",
            "  inflating: finished_files/chunked/train_049.bin  \n",
            "  inflating: finished_files/chunked/train_048.bin  \n",
            "  inflating: finished_files/chunked/train_047.bin  \n",
            "  inflating: finished_files/chunked/train_046.bin  \n",
            "  inflating: finished_files/chunked/train_045.bin  \n",
            "  inflating: finished_files/chunked/train_044.bin  \n",
            "  inflating: finished_files/chunked/train_043.bin  \n",
            "  inflating: finished_files/chunked/train_042.bin  \n",
            "  inflating: finished_files/chunked/train_041.bin  \n",
            "  inflating: finished_files/chunked/train_040.bin  \n",
            "  inflating: finished_files/chunked/train_039.bin  \n",
            "  inflating: finished_files/chunked/train_038.bin  \n",
            "  inflating: finished_files/chunked/train_037.bin  \n",
            "  inflating: finished_files/chunked/train_036.bin  \n",
            "  inflating: finished_files/chunked/train_035.bin  \n",
            "  inflating: finished_files/chunked/train_034.bin  \n",
            "  inflating: finished_files/chunked/train_033.bin  \n",
            "  inflating: finished_files/chunked/train_032.bin  \n",
            "  inflating: finished_files/chunked/train_031.bin  \n",
            "  inflating: finished_files/chunked/train_030.bin  \n",
            "  inflating: finished_files/chunked/train_029.bin  \n",
            "  inflating: finished_files/chunked/train_028.bin  \n",
            "  inflating: finished_files/chunked/train_027.bin  \n",
            "  inflating: finished_files/chunked/train_026.bin  \n",
            "  inflating: finished_files/chunked/train_025.bin  \n",
            "  inflating: finished_files/chunked/train_024.bin  \n",
            "  inflating: finished_files/chunked/train_023.bin  \n",
            "  inflating: finished_files/chunked/train_022.bin  \n",
            "  inflating: finished_files/chunked/train_021.bin  \n",
            "  inflating: finished_files/chunked/train_020.bin  \n",
            "  inflating: finished_files/chunked/train_019.bin  \n",
            "  inflating: finished_files/chunked/train_018.bin  \n",
            "  inflating: finished_files/chunked/train_017.bin  \n",
            "  inflating: finished_files/chunked/train_016.bin  \n",
            "  inflating: finished_files/chunked/train_015.bin  \n",
            "  inflating: finished_files/chunked/train_014.bin  \n",
            "  inflating: finished_files/chunked/train_013.bin  \n",
            "  inflating: finished_files/chunked/train_012.bin  \n",
            "  inflating: finished_files/chunked/train_011.bin  \n",
            "  inflating: finished_files/chunked/train_010.bin  \n",
            "  inflating: finished_files/chunked/train_009.bin  \n",
            "  inflating: finished_files/chunked/train_008.bin  \n",
            "  inflating: finished_files/chunked/train_007.bin  \n",
            "  inflating: finished_files/chunked/train_006.bin  \n",
            "  inflating: finished_files/chunked/train_005.bin  \n",
            "  inflating: finished_files/chunked/train_004.bin  \n",
            "  inflating: finished_files/chunked/train_003.bin  \n",
            "  inflating: finished_files/chunked/train_002.bin  \n",
            "  inflating: finished_files/chunked/train_001.bin  \n",
            "  inflating: finished_files/chunked/train_000.bin  \n",
            "  inflating: finished_files/chunked/train_255.bin  \n",
            "  inflating: finished_files/chunked/train_254.bin  \n",
            "  inflating: finished_files/chunked/train_253.bin  \n",
            "  inflating: finished_files/chunked/train_252.bin  \n",
            "  inflating: finished_files/chunked/train_251.bin  \n",
            "  inflating: finished_files/chunked/train_250.bin  \n",
            "  inflating: finished_files/chunked/train_249.bin  \n",
            "  inflating: finished_files/chunked/train_248.bin  \n",
            "  inflating: finished_files/chunked/train_247.bin  \n",
            "  inflating: finished_files/chunked/train_246.bin  \n",
            "  inflating: finished_files/chunked/train_245.bin  \n",
            "  inflating: finished_files/chunked/train_244.bin  \n",
            "  inflating: finished_files/chunked/train_243.bin  \n",
            "  inflating: finished_files/chunked/train_242.bin  \n",
            "  inflating: finished_files/chunked/train_241.bin  \n",
            "  inflating: finished_files/chunked/train_240.bin  \n",
            "  inflating: finished_files/chunked/train_239.bin  \n",
            "  inflating: finished_files/chunked/train_238.bin  \n",
            "  inflating: finished_files/chunked/train_237.bin  \n",
            "  inflating: finished_files/chunked/train_236.bin  \n",
            "  inflating: finished_files/chunked/train_235.bin  \n",
            "  inflating: finished_files/chunked/train_234.bin  \n",
            "  inflating: finished_files/chunked/train_233.bin  \n",
            "  inflating: finished_files/chunked/train_232.bin  \n",
            "  inflating: finished_files/chunked/train_231.bin  \n",
            "  inflating: finished_files/chunked/train_230.bin  \n",
            "  inflating: finished_files/chunked/train_229.bin  \n",
            "  inflating: finished_files/chunked/train_228.bin  \n",
            "  inflating: finished_files/chunked/train_227.bin  \n",
            "  inflating: finished_files/chunked/train_226.bin  \n",
            "  inflating: finished_files/chunked/train_225.bin  \n",
            "  inflating: finished_files/chunked/train_224.bin  \n",
            "  inflating: finished_files/chunked/train_223.bin  \n",
            "  inflating: finished_files/chunked/train_222.bin  \n",
            "  inflating: finished_files/chunked/train_221.bin  \n",
            "  inflating: finished_files/chunked/train_220.bin  \n",
            "  inflating: finished_files/chunked/train_219.bin  \n",
            "  inflating: finished_files/chunked/train_218.bin  \n",
            "  inflating: finished_files/chunked/train_217.bin  \n",
            "  inflating: finished_files/chunked/train_216.bin  \n",
            "  inflating: finished_files/chunked/train_215.bin  \n",
            "  inflating: finished_files/chunked/train_214.bin  \n",
            "  inflating: finished_files/chunked/train_213.bin  \n",
            "  inflating: finished_files/chunked/train_212.bin  \n",
            "  inflating: finished_files/chunked/train_211.bin  \n",
            "  inflating: finished_files/chunked/train_210.bin  \n",
            "  inflating: finished_files/chunked/train_209.bin  \n",
            "  inflating: finished_files/chunked/train_208.bin  \n",
            "  inflating: finished_files/chunked/train_207.bin  \n",
            "  inflating: finished_files/chunked/train_206.bin  \n",
            "  inflating: finished_files/chunked/train_205.bin  \n",
            "  inflating: finished_files/chunked/train_204.bin  \n",
            "  inflating: finished_files/chunked/train_203.bin  \n",
            "  inflating: finished_files/chunked/train_202.bin  \n",
            "  inflating: finished_files/chunked/train_201.bin  \n",
            "  inflating: finished_files/chunked/train_200.bin  \n",
            "  inflating: finished_files/chunked/train_199.bin  \n",
            "  inflating: finished_files/chunked/train_198.bin  \n",
            "  inflating: finished_files/chunked/train_197.bin  \n",
            "  inflating: finished_files/chunked/train_196.bin  \n",
            "  inflating: finished_files/chunked/train_195.bin  \n",
            "  inflating: finished_files/chunked/train_194.bin  \n",
            "  inflating: finished_files/chunked/train_193.bin  \n",
            "  inflating: finished_files/chunked/train_192.bin  \n",
            "  inflating: finished_files/chunked/train_191.bin  \n",
            "  inflating: finished_files/chunked/train_190.bin  \n",
            "  inflating: finished_files/chunked/train_189.bin  \n",
            "  inflating: finished_files/chunked/train_188.bin  \n",
            "  inflating: finished_files/chunked/train_187.bin  \n",
            "  inflating: finished_files/chunked/train_186.bin  \n",
            "  inflating: finished_files/chunked/train_185.bin  \n",
            "  inflating: finished_files/chunked/train_184.bin  \n",
            "  inflating: finished_files/chunked/train_183.bin  \n",
            "  inflating: finished_files/chunked/train_182.bin  \n",
            "  inflating: finished_files/chunked/train_181.bin  \n",
            "  inflating: finished_files/chunked/train_180.bin  \n",
            "  inflating: finished_files/chunked/train_179.bin  \n",
            "  inflating: finished_files/chunked/train_178.bin  \n",
            "  inflating: finished_files/chunked/train_177.bin  \n",
            "  inflating: finished_files/chunked/train_176.bin  \n",
            "  inflating: finished_files/chunked/train_175.bin  \n",
            "  inflating: finished_files/chunked/train_174.bin  \n",
            "  inflating: finished_files/chunked/train_173.bin  \n",
            "  inflating: finished_files/chunked/train_172.bin  \n",
            "  inflating: finished_files/chunked/train_171.bin  \n",
            "  inflating: finished_files/chunked/train_170.bin  \n",
            "  inflating: finished_files/chunked/train_169.bin  \n",
            "  inflating: finished_files/chunked/train_168.bin  \n",
            "  inflating: finished_files/chunked/train_167.bin  \n",
            "  inflating: finished_files/chunked/train_166.bin  \n",
            "  inflating: finished_files/chunked/train_165.bin  \n",
            "  inflating: finished_files/chunked/train_164.bin  \n",
            "  inflating: finished_files/chunked/train_163.bin  \n",
            "  inflating: finished_files/chunked/train_162.bin  \n",
            "  inflating: finished_files/chunked/train_161.bin  \n",
            "  inflating: finished_files/chunked/train_160.bin  \n",
            "  inflating: finished_files/chunked/train_159.bin  \n",
            "  inflating: finished_files/chunked/train_158.bin  \n",
            "  inflating: finished_files/chunked/train_157.bin  \n",
            "  inflating: finished_files/chunked/train_156.bin  \n",
            "  inflating: finished_files/chunked/train_155.bin  \n",
            "  inflating: finished_files/chunked/train_154.bin  \n",
            "  inflating: finished_files/chunked/train_153.bin  \n",
            "  inflating: finished_files/chunked/train_152.bin  \n",
            "  inflating: finished_files/chunked/train_151.bin  \n",
            "  inflating: finished_files/chunked/train_150.bin  \n",
            "  inflating: finished_files/chunked/train_149.bin  \n",
            "  inflating: finished_files/chunked/train_148.bin  \n",
            "  inflating: finished_files/chunked/train_147.bin  \n",
            "  inflating: finished_files/chunked/train_146.bin  \n",
            "  inflating: finished_files/chunked/train_145.bin  \n",
            "  inflating: finished_files/chunked/train_144.bin  \n",
            "  inflating: finished_files/chunked/train_143.bin  \n",
            "  inflating: finished_files/chunked/train_142.bin  \n",
            "  inflating: finished_files/chunked/train_141.bin  \n",
            "  inflating: finished_files/chunked/train_140.bin  \n",
            "  inflating: finished_files/chunked/train_139.bin  \n",
            "  inflating: finished_files/chunked/train_138.bin  \n",
            "  inflating: finished_files/chunked/train_137.bin  \n",
            "  inflating: finished_files/chunked/train_136.bin  \n",
            "  inflating: finished_files/chunked/train_135.bin  \n",
            "  inflating: finished_files/chunked/train_134.bin  \n",
            "  inflating: finished_files/chunked/train_133.bin  \n",
            "  inflating: finished_files/chunked/train_132.bin  \n",
            "  inflating: finished_files/chunked/train_131.bin  \n",
            "  inflating: finished_files/chunked/train_130.bin  \n",
            "  inflating: finished_files/chunked/train_129.bin  \n",
            "  inflating: finished_files/chunked/train_128.bin  \n",
            "  inflating: finished_files/chunked/test_011.bin  \n",
            "  inflating: finished_files/chunked/test_010.bin  \n",
            "  inflating: finished_files/chunked/test_009.bin  \n",
            "  inflating: finished_files/chunked/test_008.bin  \n",
            "  inflating: finished_files/chunked/test_007.bin  \n",
            "  inflating: finished_files/chunked/test_006.bin  \n",
            "  inflating: finished_files/chunked/test_005.bin  \n",
            "  inflating: finished_files/chunked/test_004.bin  \n",
            "  inflating: finished_files/chunked/test_003.bin  \n",
            "  inflating: finished_files/chunked/test_002.bin  \n",
            "  inflating: finished_files/chunked/test_001.bin  \n",
            "  inflating: finished_files/chunked/test_000.bin  \n",
            "  inflating: finished_files/chunked/val_013.bin  \n",
            "  inflating: finished_files/chunked/val_012.bin  \n",
            "  inflating: finished_files/chunked/val_011.bin  \n",
            "  inflating: finished_files/chunked/val_010.bin  \n",
            "  inflating: finished_files/chunked/val_009.bin  \n",
            "  inflating: finished_files/chunked/val_008.bin  \n",
            "  inflating: finished_files/chunked/val_007.bin  \n",
            "  inflating: finished_files/chunked/val_006.bin  \n",
            "  inflating: finished_files/chunked/val_005.bin  \n",
            "  inflating: finished_files/chunked/val_004.bin  \n",
            "  inflating: finished_files/chunked/val_003.bin  \n",
            "  inflating: finished_files/chunked/val_002.bin  \n",
            "  inflating: finished_files/chunked/val_001.bin  \n",
            "  inflating: finished_files/chunked/val_000.bin  \n",
            "  inflating: finished_files/chunked/train_287.bin  \n",
            "  inflating: finished_files/chunked/train_286.bin  \n",
            "  inflating: finished_files/chunked/train_285.bin  \n",
            "  inflating: finished_files/chunked/train_284.bin  \n",
            "  inflating: finished_files/chunked/train_283.bin  \n",
            "  inflating: finished_files/chunked/train_282.bin  \n",
            "  inflating: finished_files/chunked/train_281.bin  \n",
            "  inflating: finished_files/chunked/train_280.bin  \n",
            "  inflating: finished_files/chunked/train_279.bin  \n",
            "  inflating: finished_files/chunked/train_278.bin  \n",
            "  inflating: finished_files/chunked/train_277.bin  \n",
            "  inflating: finished_files/chunked/train_276.bin  \n",
            "  inflating: finished_files/chunked/train_275.bin  \n",
            "  inflating: finished_files/chunked/train_274.bin  \n",
            "  inflating: finished_files/chunked/train_273.bin  \n",
            "  inflating: finished_files/chunked/train_272.bin  \n",
            "  inflating: finished_files/chunked/train_271.bin  \n",
            "  inflating: finished_files/chunked/train_270.bin  \n",
            "  inflating: finished_files/chunked/train_269.bin  \n",
            "  inflating: finished_files/chunked/train_268.bin  \n",
            "  inflating: finished_files/chunked/train_267.bin  \n",
            "  inflating: finished_files/chunked/train_266.bin  \n",
            "  inflating: finished_files/chunked/train_265.bin  \n",
            "  inflating: finished_files/chunked/train_264.bin  \n",
            "  inflating: finished_files/chunked/train_263.bin  \n",
            "  inflating: finished_files/chunked/train_262.bin  \n",
            "  inflating: finished_files/chunked/train_261.bin  \n",
            "  inflating: finished_files/chunked/train_260.bin  \n",
            "  inflating: finished_files/chunked/train_259.bin  \n",
            "  inflating: finished_files/chunked/train_258.bin  \n",
            "  inflating: finished_files/chunked/train_257.bin  \n",
            "  inflating: finished_files/chunked/train_256.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CSNj8iTPI5Us",
        "colab_type": "code",
        "outputId": "69efe853-4d7b-4aca-8b58-e7a9d2954d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip \"drive/My Drive/Assignment 4 Chatbot/cnn_stories_tokenized.zip\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/Assignment 4 Chatbot/cnn_stories_tokenized.zip\n",
            "replace cnn_stories_tokenized/daf94a7a113ecd9303f4ab32cea2217d03fe35bb.story? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4h7jH6BLrlvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YINCo-Y3PF6b",
        "colab_type": "code",
        "outputId": "069bf1ce-b628-4de8-c2e6-4611dff32d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "get_art_abs(\"drive/My Drive/Colab Notebooks/data/000cd1ee0098c4d510a03ddc97d11764448ebac2.story\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"kathmandu , nepal -lrb- cnn -rrb- -- a ferocious leopard may have killed 15 people in nepal in a 15-month span , its latest victim a 4-year-old boy that the creature dragged away into the jungle to eat . the head of boy was found in the forest a kilometer from his home saturday morning , said kamal prasad kharel , the police chief of the baitadi district , an area about 600 kilometers -lrb- 373 miles -rrb- west of kathmandu . the grisly discovery , which came after teams of people searched for the child , marks the 15th victim in the past 15 months in that remote district in western nepal . the police chief suspects that a single man-eating leopard is responsible for the deaths . if not , there are at most two of the man-eating creatures around , he believes . maheshwor dhakal , an ecologist at the department of national parks and wildlife conservation in kathmandu , agreed that it is unusual to find more than one or two man-eating animals in one area . most leopards live on wild prey . more human victims could also be expected if there were more than one or two man-eaters around , he said . `` since human blood has more salt than animal blood , once wild animals get the taste of salty blood they do not like other animals like deer , '' dhakal said . kharel said he feared the actual number of people killed by the leopard could be higher than 15 , because others have lost their life to leopard attacks in uttarkhand state in northern india , which borders baitadi district . `` it could be the same leopard , '' he said . of the 15 victims in nepal so far , two-thirds are children below the age of 10 . the others are older children and a 29-year-old woman who had gone to collect fodder for domestic animals in the nearby forest , a common practice in nepal . `` no adult male has been killed , '' kharel said . all the victims are from villages bordering the dense forests in the district , he said . after killing its victim , the leopard takes the body away into the forest to eat . `` in the case of the children it just leaves behind the head , eating everything , but some parts of the adult body are left behind because it can not finish it , '' kharel added . the district administration has announced a rs . 25,000 -lrb- about $ 300 -rrb- reward to anyone who captures or kills the leopard . the local administration has sought to raise public awareness of the dangers of going alone into nearby forests and has mobilized the police , armed police force and local people who have licensed guns to hunt for the animal . controlling this particular leopard has been a challenge for the wildlife officials in kathmandu . `` we are sending a veterinary doctor to the district to understand the situation , '' dhakal , the ecologist , said . `` there is no alternative but to kill the leopard . '' the chief district administrator has granted permission for this particular leopard to be killed . normally , it is illegal to kill wild animals . leopards are common in the low mountain areas , as compared to the high himalayas , across the country . while cases of leopards killing domestic animals are common , and there are sometimes instances of leopards killing people in nepal , this case is `` extreme , '' dhakal said .\",\n",
              " 'a 4-year-old boy is the latest victim of a man-eating leopard , a local police chief says . he suspects one leopard is behind the deaths of 15 people in the past 15 months . a reward has been offered to anyone who captures or kills the man-eating creature . leopards are common in low mountain areas of nepal but usually eat wild prey like deer .')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "84YPV7lyo3oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(cnn_content_file,'w',encoding='utf-8') as content, open(cnn_summary_file,'w',encoding='utf-8') as summary:\n",
        "        for s in cnn_stories:\n",
        "            s_file = os.path.join(os.path.join(os.getcwd(),cnn_tokenized_stories_dir,s))\n",
        "            article,abstract = get_art_abs(s_file)\n",
        "            if(len(article.split())!=0):\n",
        "              content.write(article.rstrip('\\n')+'\\n')\n",
        "              summary.write(abstract.rstrip('\\n')+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpZe_figPF9s",
        "colab_type": "code",
        "outputId": "85f6b253-8e33-44a1-b80b-80adcc094f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "article,abstract = get_art_abs(\"drive/My Drive/Colab Notebooks/data/000cd1ee0098c4d510a03ddc97d11764448ebac2.story\")\n",
        "\n",
        "if(len(article.split())!=0):\n",
        "  content.write(article.rstrip('\\n')+'\\n')\n",
        "  summary.write(abstract.rstrip('\\n')+'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8e65d754ae3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r4UEM3TDBc3P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWl9lU9HDQ43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We use these to separate the summary sentences in the .bin datafiles\n",
        "SENTENCE_START = '<s>'\n",
        "SENTENCE_END = '</s>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWsdLo2FDQ-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_tokenized_stories_dir = \"cnn_stories_tokenized\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NbhmzLnDRDR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are the number of .story files we expect there to be in cnn_stories_dir\n",
        "# num_expected_cnn_stories = 92579"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "18J2tug6EOGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_stories(stories_dir, tokenized_stories_dir):\n",
        "  \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
        "  print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
        "  \n",
        "  stories = os.listdir(stories_dir)\n",
        "  # make IO list file\n",
        "  print(\"Making list of files to tokenize...\")\n",
        "  \n",
        "  with open(\"mapping.txt\", \"w\") as f:\n",
        "    for s in stories:\n",
        "      f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
        "  command = ['java', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
        "  print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
        "  \n",
        "  subprocess.call(command)\n",
        "  print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
        "  \n",
        "  os.remove(\"mapping.txt\")\n",
        "\n",
        "  # Check that the tokenized stories directory contains the same number of files as the original directory\n",
        "  #num_orig = len(os.listdir(stories_dir))\n",
        "  #num_tokenized = len(os.listdir(tokenized_stories_dir))\n",
        "  \n",
        "  if num_orig != num_tokenized:\n",
        "    raise Exception(\"The tokenized stories directory %s contains %i files, but it should contain the same number as %s (which has %i files). Was there an error during tokenization?\" % (tokenized_stories_dir, num_tokenized, stories_dir, num_orig))\n",
        "  \n",
        "  print(\"Successfully finished tokenizing %s to %s.\\n\" % (stories_dir, tokenized_stories_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yFkf3SjpEn7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_file(text_file):\n",
        "  lines = []\n",
        "  with open(text_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      lines.append(line.strip())\n",
        "  return lines\n",
        "\n",
        "def fix_missing_period(line):\n",
        "  \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
        "  if \"@highlight\" in line: return line\n",
        "  if line==\"\": return line\n",
        "  if line[-1] in END_TOKENS: return line\n",
        "  return line + \" .\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0h6ZVvUPEvj4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_art_abs(story_file):\n",
        "  lines = read_text_file(story_file)\n",
        "\n",
        "  # Lowercase everything\n",
        "  lines = [line.lower() for line in lines]\n",
        "\n",
        "  '''\n",
        "  Put periods on the ends of lines that are missing them \n",
        "  this is a problem in the dataset because many image captions don't end in periods\n",
        "  consequently they end up in the body of the article as run-on sentences\n",
        "  '''\n",
        "  lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "  # Separate out article and abstract sentences\n",
        "  article_lines = []\n",
        "  highlights = []\n",
        "  next_is_highlight = False\n",
        "  \n",
        "  for idx,line in enumerate(lines):\n",
        "    if line == \"\":\n",
        "      continue # empty line\n",
        "      \n",
        "    elif line.startswith(\"@highlight\"):\n",
        "      next_is_highlight = True\n",
        "      \n",
        "    elif next_is_highlight:\n",
        "      highlights.append(line)\n",
        "      \n",
        "    else:\n",
        "      article_lines.append(line)\n",
        "\n",
        "  # Make article into a single string\n",
        "  article = ' '.join(article_lines)\n",
        "\n",
        "  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
        "  abstract = ' '.join(highlights)\n",
        "\n",
        "  return article, abstract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gu6deGwhFNup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "def check_num_stories(stories_dir, num_expected):\n",
        "  num_stories = len(os.listdir(stories_dir))\n",
        "  \n",
        "  if num_stories != num_expected:\n",
        "    raise Exception(\"stories directory %s contains %i files but should contain %i\" % (stories_dir, num_stories, num_expected))\n",
        "    \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DN_ECsfTFSQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_corpus(cnn_tokenized_stories_dir, cnn_corpus_dir):\n",
        "    cnn_stories = os.listdir(cnn_tokenized_stories_dir)\n",
        "    \n",
        "    cnn_content_file = os.path.join(cnn_corpus_dir,'articles.txt')\n",
        "    cnn_summary_file = os.path.join(cnn_corpus_dir,'abstracts.txt')\n",
        "\n",
        "    with open(cnn_content_file,'w',encoding='utf-8') as content, open(cnn_summary_file,'w',encoding='utf-8') as summary:\n",
        "        for s in cnn_stories:\n",
        "            s_file = os.path.join(os.path.join(os.getcwd(),cnn_tokenized_stories_dir,s))\n",
        "            article,abstract = get_art_abs(s_file)\n",
        "            if(len(article.split())!=0):\n",
        "              content.write(article.rstrip('\\n')+'\\n')\n",
        "              summary.write(abstract.rstrip('\\n')+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEq40gkFFaOy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_corpus_dir = \"cnn_stories_tokenized\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IFUaWoXx1hR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create_corpus(cnn_tokenized_stories_dir, cnn_corpus_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eo152vTUyRrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff1e49a0-d349-4a58-ac4a-895291a6fe0f"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcnn_stories_tokenized\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfinished_files\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}